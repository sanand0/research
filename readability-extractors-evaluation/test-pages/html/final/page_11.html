<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = 'https://distill.pub/template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>


<style id="distill-prerendered-styles" type="text/css">/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

html {
  font-size: 14px;
	line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
	border-collapse: collapse;
	border-spacing: 0;
}

table th {
	text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: style;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style>

<!-- Import Vega & Vega-Lite (does not have to be from CDN) -->
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4"></script>
<!-- Import vega-embed -->
<script src="https://cdn.jsdelivr.net/npm/vega-embed@v6"></script>


<!-- Mathjax -->
<script async="" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

<!-- End Mathjax -->

    
    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=
">
    <link href="/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
  
    <title>A Gentle Introduction to Graph Neural Networks</title>
    
    <link rel="canonical" href="https://distill.pub/2021/gnn-intro">
    
    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="What components are needed for building learning algorithms that leverage the structure and properties of graphs?">
    <meta property="article:published" itemprop="datePublished" content="2021-09-02">
    <meta property="article:created" itemprop="dateCreated" content="2021-09-02">
    
    <meta property="article:modified" itemprop="dateModified" content="2021-09-08T21:19:59.000Z">
    
    <meta property="article:author" content="Benjamin Sanchez-Lengeling">
    <meta property="article:author" content="Emily Reif">
    <meta property="article:author" content="Adam Pearce">
    <meta property="article:author" content="Alexander B. Wiltschko">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="A Gentle Introduction to Graph Neural Networks">
    <meta property="og:description" content="What components are needed for building learning algorithms that leverage the structure and properties of graphs?">
    <meta property="og:url" content="https://distill.pub/2021/gnn-intro">
    <meta property="og:image" content="https://distill.pub/2021/gnn-intro/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="A Gentle Introduction to Graph Neural Networks">
    <meta name="twitter:description" content="What components are needed for building learning algorithms that leverage the structure and properties of graphs?">
    <meta name="twitter:url" content="https://distill.pub/2021/gnn-intro">
    <meta name="twitter:image" content="https://distill.pub/2021/gnn-intro/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="A Gentle Introduction to Graph Neural Networks">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2021/gnn-intro">
    <meta name="citation_volume" content="6">
    <meta name="citation_issue" content="9">
    <meta name="citation_firstpage" content="e33">
    <meta name="citation_doi" content="10.23915/distill.00033">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2021/09/02">
    <meta name="citation_publication_date" content="2021/09/02">
    <meta name="citation_author" content="Sanchez-Lengeling, Benjamin">
    <meta name="citation_author" content="Reif, Emily">
    <meta name="citation_author" content="Pearce, Adam">
    <meta name="citation_author" content="Wiltschko, Alexander B.">
    <meta name="citation_reference" content="citation_title=Understanding Convolutions on Graphs;citation_author=Ameya Daigavane;citation_author=Balaraman Ravindran;citation_author=Gaurav Aggarwal;citation_publication_date=2021;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=The Graph Neural Network Model;citation_author=F Scarselli;citation_author=M Gori;citation_author=Ah Chung Tsoi;citation_author=M Hagenbuchner;citation_author=G Monfardini;citation_publication_date=2009;citation_journal_title=IEEE Transactions on Neural Networks;citation_volume=20;citation_number=1;">
    <meta name="citation_reference" content="citation_title=A Deep Learning Approach to Antibiotic Discovery;citation_author=Jonathan M Stokes;citation_author=Kevin Yang;citation_author=Kyle Swanson;citation_author=Wengong Jin;citation_author=Andres Cubillos-Ruiz;citation_author=Nina M Donghia;citation_author=Craig R MacNair;citation_author=Shawn French;citation_author=Lindsey A Carfrae;citation_author=Zohar Bloom-Ackermann;citation_author=Victoria M Tran;citation_author=Anush Chiappino-Pepe;citation_author=Ahmed H Badran;citation_author=Ian W Andrews;citation_author=Emma J Chory;citation_author=George M Church;citation_author=Eric D Brown;citation_author=Tommi S Jaakkola;citation_author=Regina Barzilay;citation_author=James J Collins;citation_publication_date=2020;citation_journal_title=Cell;citation_volume=181;citation_number=2;">
    <meta name="citation_reference" content="citation_title=Learning to simulate complex physics with graph networks;citation_author=Alvaro Sanchez-Gonzalez;citation_author=Jonathan Godwin;citation_author=Tobias Pfaff;citation_author=Rex Ying;citation_author=Jure Leskovec;citation_author=Peter W Battaglia;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Fake News Detection on Social Media using Geometric Deep Learning;citation_author=Federico Monti;citation_author=Fabrizio Frasca;citation_author=Davide Eynard;citation_author=Damon Mannion;citation_author=Michael M Bronstein;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Traffic prediction with advanced Graph Neural Networks;citation_author=Oliver Lange *;citation_author=Luis Perez;">
    <meta name="citation_reference" content="citation_title=Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in {Real-Time};citation_author=Chantat Eksombatchai;citation_author=Pranav Jindal;citation_author=Jerry Zitao Liu;citation_author=Yuchen Liu;citation_author=Rahul Sharma;citation_author=Charles Sugnet;citation_author=Mark Ulrich;citation_author=Jure Leskovec;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Convolutional Networks on Graphs for Learning Molecular Fingerprints;citation_author=David Duvenaud;citation_author=Dougal Maclaurin;citation_author=Jorge Aguilera-Iparraguirre;citation_author=Rafael Gomez-Bombarelli;citation_author=Timothy Hirzel;citation_author=Alan Aspuru-Guzik;citation_author=Ryan P Adams;citation_publication_date=2015;">
    <meta name="citation_reference" content="citation_title=Distributed Representations of Words and Phrases and their Compositionality;citation_author=Tomas Mikolov;citation_author=Ilya Sutskever;citation_author=Kai Chen;citation_author=Greg Corrado;citation_author=Jeffrey Dean;citation_publication_date=2013;">
    <meta name="citation_reference" content="citation_title=BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding;citation_author=Jacob Devlin;citation_author=Ming-Wei Chang;citation_author=Kenton Lee;citation_author=Kristina Toutanova;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Glove: Global Vectors for Word Representation;citation_author=Jeffrey Pennington;citation_author=Richard Socher;citation_author=Christopher Manning;citation_publication_date=2014;citation_journal_title=Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP);">
    <meta name="citation_reference" content="citation_title=Learning to Represent Programs with Graphs;citation_author=Miltiadis Allamanis;citation_author=Marc Brockschmidt;citation_author=Mahmoud Khademi;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Deep Learning for Symbolic Mathematics;citation_author=Guillaume Lample;citation_author=Francois Charton;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=KONECT;citation_author=Jerome Kunegis;citation_publication_date=2013;citation_journal_title=Proceedings of the 22nd International Conference on World Wide Web - WWW &amp;#39;13 Companion;">
    <meta name="citation_reference" content="citation_title=An Information Flow Model for Conflict and Fission in Small Groups;citation_author=Wayne W Zachary;citation_publication_date=1977;citation_journal_title=J. Anthropol. Res.;citation_volume=33;citation_number=4;">
    <meta name="citation_reference" content="citation_title=Learning Latent Permutations with Gumbel-Sinkhorn Networks;citation_author=Gonzalo Mena;citation_author=David Belanger;citation_author=Scott Linderman;citation_author=Jasper Snoek;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs;citation_author=Ryan L Murphy;citation_author=Balasubramaniam Srinivasan;citation_author=Vinayak Rao;citation_author=Bruno Ribeiro;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Neural Message Passing for Quantum Chemistry;citation_author=Justin Gilmer;citation_author=Samuel S Schoenholz;citation_author=Patrick F Riley;citation_author=Oriol Vinyals;citation_author=George E Dahl;citation_publication_date=2017;citation_volume=70;">
    <meta name="citation_reference" content="citation_title=Relational inductive biases, deep learning, and graph networks;citation_author=Peter W Battaglia;citation_author=Jessica B Hamrick;citation_author=Victor Bapst;citation_author=Alvaro Sanchez-Gonzalez;citation_author=Vinicius Zambaldi;citation_author=Mateusz Malinowski;citation_author=Andrea Tacchetti;citation_author=David Raposo;citation_author=Adam Santoro;citation_author=Ryan Faulkner;citation_author=Caglar Gulcehre;citation_author=Francis Song;citation_author=Andrew Ballard;citation_author=Justin Gilmer;citation_author=George Dahl;citation_author=Ashish Vaswani;citation_author=Kelsey Allen;citation_author=Charles Nash;citation_author=Victoria Langston;citation_author=Chris Dyer;citation_author=Nicolas Heess;citation_author=Daan Wierstra;citation_author=Pushmeet Kohli;citation_author=Matt Botvinick;citation_author=Oriol Vinyals;citation_author=Yujia Li;citation_author=Razvan Pascanu;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Deep Sets;citation_author=Manzil Zaheer;citation_author=Satwik Kottur;citation_author=Siamak Ravanbakhsh;citation_author=Barnabas Poczos;citation_author=Ruslan Salakhutdinov;citation_author=Alexander Smola;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Molecular graph convolutions: moving beyond fingerprints;citation_author=Steven Kearnes;citation_author=Kevin McCloskey;citation_author=Marc Berndl;citation_author=Vijay Pande;citation_author=Patrick Riley;citation_publication_date=2016;citation_journal_title=J. Comput. Aided Mol. Des.;citation_volume=30;citation_number=8;">
    <meta name="citation_reference" content="citation_title=Feature-wise transformations;citation_author=Vincent Dumoulin;citation_author=Ethan Perez;citation_author=Nathan Schucher;citation_author=Florian Strub;citation_author=Harm de Vries;citation_author=Aaron Courville;citation_author=Yoshua Bengio;citation_publication_date=2018;citation_journal_title=Distill;citation_volume=3;citation_number=7;">
    <meta name="citation_reference" content="citation_title=Leffingwell Odor Dataset;citation_author=Benjamin Sanchez-Lengeling;citation_author=Jennifer N Wei;citation_author=Brian K Lee;citation_author=Richard C Gerkin;citation_author=Alan Aspuru-Guzik;citation_author=Alexander B Wiltschko;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules;citation_author=Benjamin Sanchez-Lengeling;citation_author=Jennifer N Wei;citation_author=Brian K Lee;citation_author=Richard C Gerkin;citation_author=Alan Aspuru-Guzik;citation_author=Alexander B Wiltschko;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Benchmarking Graph Neural Networks;citation_author=Vijay Prakash Dwivedi;citation_author=Chaitanya K Joshi;citation_author=Thomas Laurent;citation_author=Yoshua Bengio;citation_author=Xavier Bresson;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Design Space for Graph Neural Networks;citation_author=Jiaxuan You;citation_author=Rex Ying;citation_author=Jure Leskovec;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Principal Neighbourhood Aggregation for Graph Nets;citation_author=Gabriele Corso;citation_author=Luca Cavalleri;citation_author=Dominique Beaini;citation_author=Pietro Lio;citation_author=Petar Velickovic;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning;citation_author=Elan Markowitz;citation_author=Keshav Balasubramanian;citation_author=Mehrnoosh Mirtaheri;citation_author=Sami Abu-El-Haija;citation_author=Bryan Perozzi;citation_author=Greg Ver Steeg;citation_author=Aram Galstyan;citation_publication_date=2021;">
    <meta name="citation_reference" content="citation_title=Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels;citation_author=Simon S Du;citation_author=Kangcheng Hou;citation_author=Barnabas Poczos;citation_author=Ruslan Salakhutdinov;citation_author=Ruosong Wang;citation_author=Keyulu Xu;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Representation Learning on Graphs with Jumping Knowledge Networks;citation_author=Keyulu Xu;citation_author=Chengtao Li;citation_author=Yonglong Tian;citation_author=Tomohiro Sonobe;citation_author=Ken-Ichi Kawarabayashi;citation_author=Stefanie Jegelka;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Neural Execution of Graph Algorithms;citation_author=Petar Velickovic;citation_author=Rex Ying;citation_author=Matilde Padovano;citation_author=Raia Hadsell;citation_author=Charles Blundell;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Graph Theory;citation_author=Frank Harary;citation_publication_date=1969;">
    <meta name="citation_reference" content="citation_title=A nested-graph model for the representation and manipulation of complex objects;citation_author=Alexandra Poulovassilis;citation_author=Mark Levene;citation_publication_date=1994;citation_journal_title=ACM Transactions on Information Systems;citation_volume=12;citation_number=1;">
    <meta name="citation_reference" content="citation_title=Modeling polypharmacy side effects with graph convolutional networks;citation_author=Marinka Zitnik;citation_author=Monica Agrawal;citation_author=Jure Leskovec;citation_publication_date=2018;citation_journal_title=Bioinformatics;citation_volume=34;citation_number=13;">
    <meta name="citation_reference" content="citation_title=Machine learning in chemical reaction space;citation_author=Sina Stocker;citation_author=Gabor Csanyi;citation_author=Karsten Reuter;citation_author=Johannes T Margraf;citation_publication_date=2020;citation_journal_title=Nat. Commun.;citation_volume=11;citation_number=1;">
    <meta name="citation_reference" content="citation_title=Graphs and Hypergraphs;citation_author=Claude Berge;citation_publication_date=1976;">
    <meta name="citation_reference" content="citation_title=HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs;citation_author=Naganand Yadati;citation_author=Madhav Nimishakavi;citation_author=Prateek Yadav;citation_author=Vikram Nitin;citation_author=Anand Louis;citation_author=Partha Talukdar;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Hierarchical Message-Passing Graph Neural Networks;citation_author=Zhiqiang Zhong;citation_author=Cheng-Te Li;citation_author=Jun Pang;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Little Ball of Fur;citation_author=Benedek Rozemberczki;citation_author=Oliver Kiss;citation_author=Rik Sarkar;citation_publication_date=2020;citation_journal_title=Proceedings of the 29th ACM International Conference on Information &amp;amp; Knowledge Management;">
    <meta name="citation_reference" content="citation_title=Sampling from large graphs;citation_author=Jure Leskovec;citation_author=Christos Faloutsos;citation_publication_date=2006;citation_journal_title=Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD &amp;#39;06;">
    <meta name="citation_reference" content="citation_title=Metropolis Algorithms for Representative Subgraph Sampling;citation_author=Christian Hubler;citation_author=Hans-Peter Kriegel;citation_author=Karsten Borgwardt;citation_author=Zoubin Ghahramani;citation_publication_date=2008;citation_journal_title=2008 Eighth IEEE International Conference on Data Mining;">
    <meta name="citation_reference" content="citation_title=Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks;citation_author=Wei-Lin Chiang;citation_author=Xuanqing Liu;citation_author=Si Si;citation_author=Yang Li;citation_author=Samy Bengio;citation_author=Cho-Jui Hsieh;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=GraphSAINT: Graph Sampling Based Inductive Learning Method;citation_author=Hanqing Zeng;citation_author=Hongkuan Zhou;citation_author=Ajitesh Srivastava;citation_author=Rajgopal Kannan;citation_author=Viktor Prasanna;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=How Powerful are Graph Neural Networks?;citation_author=Keyulu Xu;citation_author=Weihua Hu;citation_author=Jure Leskovec;citation_author=Stefanie Jegelka;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Rep the Set: Neural Networks for Learning Set Representations;citation_author=Konstantinos Skianis;citation_author=Giannis Nikolentzos;citation_author=Stratis Limnios;citation_author=Michalis Vazirgiannis;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=Message Passing Networks for Molecules with Tetrahedral Chirality;citation_author=Lagnajit Pattanaik;citation_author=Octavian-Eugen Ganea;citation_author=Ian Coley;citation_author=Klavs F Jensen;citation_author=William H Green;citation_author=Connor W Coley;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules;citation_author=Shengchao Liu;citation_author=Mehmet Furkan Demirel;citation_author=Yingyu Liang;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Dual-Primal Graph Convolutional Networks;citation_author=Federico Monti;citation_author=Oleksandr Shchur;citation_author=Aleksandar Bojchevski;citation_author=Or Litany;citation_author=Stephan Gunnemann;citation_author=Michael M Bronstein;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Viewing matrices &amp; probability as graphs;citation_author=Tai-Danae Bradley;">
    <meta name="citation_reference" content="citation_title=Graphs and Matrices;citation_author=Ravindra B Bapat;citation_publication_date=2014;">
    <meta name="citation_reference" content="citation_title=Modern Graph Theory;citation_author=Bela Bollobas;citation_publication_date=2013;">
    <meta name="citation_reference" content="citation_title=Attention Is All You Need;citation_author=Ashish Vaswani;citation_author=Noam Shazeer;citation_author=Niki Parmar;citation_author=Jakob Uszkoreit;citation_author=Llion Jones;citation_author=Aidan N Gomez;citation_author=Lukasz Kaiser;citation_author=Illia Polosukhin;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Graph Attention Networks;citation_author=Petar Velickovic;citation_author=Guillem Cucurull;citation_author=Arantxa Casanova;citation_author=Adriana Romero;citation_author=Pietro Lio;citation_author=Yoshua Bengio;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks;citation_author=Juho Lee;citation_author=Yoonho Lee;citation_author=Jungtaek Kim;citation_author=Adam R Kosiorek;citation_author=Seungjin Choi;citation_author=Yee Whye Teh;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Transformers are Graph Neural Networks;citation_author=Chaitanya Joshi;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Using Attribution to Decode Dataset Bias in Neural Network Models for Chemistry;citation_author=Kevin McCloskey;citation_author=Ankur Taly;citation_author=Federico Monti;citation_author=Michael P Brenner;citation_author=Lucy Colwell;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=GNNExplainer: Generating Explanations for Graph Neural Networks;citation_author=Zhitao Ying;citation_author=Dylan Bourgeois;citation_author=Jiaxuan You;citation_author=Marinka Zitnik;citation_author=Jure Leskovec;citation_publication_date=2019;citation_volume=32;">
    <meta name="citation_reference" content="citation_title=Explainability Methods for Graph Convolutional Neural Networks;citation_author=Phillip E Pope;citation_author=Soheil Kolouri;citation_author=Mohammad Rostami;citation_author=Charles E Martin;citation_author=Heiko Hoffmann;citation_publication_date=2019;citation_journal_title=2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);">
    <meta name="citation_reference" content="citation_title=Evaluating Attribution for Graph Neural Networks;citation_author=Benjamin Sanchez-Lengeling;citation_author=Jennifer Wei;citation_author=Brian Lee;citation_author=Emily Reif;citation_author=Wesley Qian;citation_author=Yiliu Wang;citation_author=Kevin James McCloskey;citation_author=Lucy Colwell;citation_author=Alexander B Wiltschko;citation_publication_date=2020;">
    <meta name="citation_reference" content="citation_title=Variational Graph Auto-Encoders;citation_author=Thomas N Kipf;citation_author=Max Welling;citation_publication_date=2016;">
    <meta name="citation_reference" content="citation_title=GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models;citation_author=Jiaxuan You;citation_author=Rex Ying;citation_author=Xiang Ren;citation_author=William L Hamilton;citation_author=Jure Leskovec;citation_publication_date=2018;">
    <meta name="citation_reference" content="citation_title=Optimization of Molecules via Deep Reinforcement Learning;citation_author=Zhenpeng Zhou;citation_author=Steven Kearnes;citation_author=Li Li;citation_author=Richard N Zare;citation_author=Patrick Riley;citation_publication_date=2019;citation_journal_title=Sci. Rep.;citation_volume=9;citation_number=1;">
    <meta name="citation_reference" content="citation_title=Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation;citation_author=Mario Krenn;citation_author=Florian Hase;citation_author=Akshatkumar Nigam;citation_author=Pascal Friederich;citation_author=Alan Aspuru-Guzik;citation_publication_date=2019;">
    <meta name="citation_reference" content="citation_title=GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation;citation_author=Nikhil Goyal;citation_author=Harsh Vardhan Jain;citation_author=Sayan Ranu;citation_publication_date=2020;">
</head>
<body distill-prerendered=""><distill-header distill-prerendered="">
<style>
distill-header {
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
distill-header .content {
  height: 70px;
  grid-column: page;
}
distill-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
distill-header a:hover {
  color: rgba(255, 255, 255, 1);
}
distill-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  distill-header {
    height: 70px;
  }
  distill-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  distill-header .logo {
  }
}
distill-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
distill-header .logo {
  font-size: 17px;
  font-weight: 200;
}
distill-header .nav {
  float: right;
  font-weight: 300;
}
distill-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>
<div class="content">
  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  <nav class="nav">
    <a href="/about/">About</a>
    <a href="/prize/">Prize</a>
    <a href="/journal/">Submit</a>
  </nav>
</div>
</distill-header>
<title>A Gentle Introduction to Graph Neural Networks</title>

<link rel="stylesheet" href="style.e308ff8e.css">
<link rel="stylesheet" href="layerwise_trace.d38144f3.css">
<link rel="stylesheet" href="shuffle.90a7da43.css">
<link rel="stylesheet" href="text-as-graph.146f5ac8.css">
<link rel="stylesheet" href="pca-layers.889c7e67.css">
<link rel="stylesheet" href="gnn-playground.7fda89e6.css">
<link rel="stylesheet" href="mols-as-graph.f8d43714.css">
<link rel="stylesheet" href="shuffle-sm.e4a48acb.css">
<link rel="stylesheet" href="table.0b6daf45.css">
<link rel="stylesheet" href="graph-description.c0d85959.css">
<link rel="stylesheet" href="graph-description-embeddings.95e72025.css">


<d-front-matter>
  <script type="text/json">
    {
    "title": "A Gentle Introduction to Graph Neural Networks",
    "description": "What components are needed for building learning algorithms that leverage the structure and properties of graphs?",
    "authors": [{
      "author": "Benjamin Sanchez-Lengeling",
      "affiliations": [{
        "name": "Google Research",
        "affiliationURL": "https://research.google/teams/brain/"
      }]
    }, {
      "author": "Emily Reif",
      "affiliations": [{
        "name": "Google Research",
        "affiliationURL": "https://research.google/teams/brain/"
      }]
    }, {
      "author": "Adam Pearce",
      "authorURL": "https://roadtolarissa.com",
      "affiliations": [{
        "name": "Google Research",
        "affiliationURL": "https://research.google/teams/brain/"
      }]
    }, {
      "author": "Alexander B. Wiltschko",
      "affiliations": [{
        "name": "Google Research",
        "affiliationURL": "https://research.google/teams/brain/"
      }]
    }]
    }
  </script>
</d-front-matter>




<d-title>
  <h1>A Gentle Introduction to Graph Neural Networks</h1>
  <p>Neural networks have been adapted to leverage the structure and properties of graphs. We explore the components needed for building a graph neural network - and motivate the design choices behind them.</p>

  <figure class="teaser">
    <div id="layerwise-trace"> </div>
    <figcaption>
Hover over a node in the diagram below to see how it accumulates information from nodes around it through the layers of the network.
    </figcaption>
</figure>
</d-title>
<d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
      
        <p class="author">
          
            <span class="name">Benjamin Sanchez-Lengeling</span>
        </p>
        <p class="affiliation">
        <span class="affiliation">Google Research</span>
        </p>
      
        <p class="author">
          
            <span class="name">Emily Reif</span>
        </p>
        <p class="affiliation">
        <span class="affiliation">Google Research</span>
        </p>
      
        <p class="author">
          
            <a class="name" href="https://roadtolarissa.com">Adam Pearce</a>
        </p>
        <p class="affiliation">
        <span class="affiliation">Google Research</span>
        </p>
      
        <p class="author">
          
            <span class="name">Alexander B. Wiltschko</span>
        </p>
        <p class="affiliation">
        <span class="affiliation">Google Research</span>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p>Sept. 2, 2021</p> 
    </div>
    <div>
      <h3>DOI</h3>
      
        <p><a href="https://doi.org/10.23915/distill.00033">10.23915/distill.00033</a></p>
    </div>
  </div>
</d-byline>

<d-article>

<p><em>This article is one of two Distill publications about graph neural networks. Take a look at <a href="https://distill.pub/2021/understanding-gnns/">Understanding Convolutions on Graphs</a><d-cite key="daigavane2021understanding"></d-cite> to understand how convolutions over images generalize naturally to convolutions over graphs.</em></p>
<p>Graphs are all around us; real world objects are often defined in terms of their connections to other things. A set of objects, and the connections between them, are naturally expressed as a <em>graph</em>. Researchers have developed neural networks that operate on graph data (called graph neural networks, or GNNs) for over a decade<d-cite key="Scarselli2009-ku"></d-cite>. Recent developments have increased their capabilities and expressive power. We are starting to see practical applications in areas such as antibacterial discovery <d-cite key="Stokes2020-az"></d-cite>, physics simulations  <d-cite key="Sanchez-Gonzalez2020-yo"></d-cite>, fake news detection <d-cite key="Monti2019-tf"></d-cite>, traffic prediction <d-cite key="undated-sy"></d-cite> and recommendation systems <d-cite key="Eksombatchai2017-il"></d-cite>.</p>
<p>This article explores and explains modern graph neural networks. We divide this work into four parts. First, we look at what kind of data is most naturally phrased as a graph, and some common examples. Second, we explore what makes graphs different from other types of data, and some of the specialized choices we have to make when using graphs. Third, we build a modern GNN, walking through each of the parts of the model, starting with historic modeling innovations in the field. We move gradually from a bare-bones implementation to a state-of-the-art GNN model. Fourth and finally, we provide a GNN playground where you can play around with a real-word task and dataset to build a stronger intuition of how each component of a GNN model contributes to the predictions it makes.</p>
<p>To start, letâ€™s establish what a graph is. A graph represents the relations (<em>edges</em>) between a collection of entities (<em>nodes</em>). </p>
<figure><div id="graph-description" style="margin-bottom:0.25cm;"></div>
<figcaption>
Three types of attributes we might find in a graph, hover over to highlight each attribute. Other types of graphs and attributes are explored in the <a href="#other-types-of-graphs-multigraphs-hypergraphs-hypernodes">Other types of graphs</a> section.
</figcaption></figure>


<p>To further describe each node, edge or the entire graph, we can store information in each of these pieces of the graph. </p>
<figure><div id="graph-description-embeddings"></div>
<figcaption>
Information in the form of scalars or embeddings can be stored at each graph node (left) or edge (right).
</figcaption></figure>

<p>We can additionally specialize graphs by associating directionality to edges (<em>directed, undirected</em>). </p>
<figure><img src="directed_undirected.e4b1689d.png" '="">
<figcaption>
The edges can be directed, where an edge $e$ has a source node, $v_{src}$, and a destination node $v_{dst}$. In this case, information flows from $v_{src}$ to $v_{dst}$. They can also be undirected, where there is no notion of source or destination nodes, and information flows both directions. Note that having a single undirected edge is equivalent to having one directed edge from $v_{src}$ to $v_{dst}$, and another directed edge from $v_{dst}$ to $v_{src}$.
</figcaption></figure>

<p>Graphs are very flexible data structures, and if this seems abstract now, we will make it concrete with examples in the next section. </p>
<h2 id="graphs-and-where-to-find-them">Graphs and where to find them</h2>
<p>Youâ€™re probably already familiar with some types of graph data, such as social networks. However, graphs are an extremely powerful and general representation of data, we will show two types of data that you might not think could be modeled as graphs: images and text. Although counterintuitive, one can learn more about the symmetries and structure of images and text by viewing them as graphs,, and build an intuition that will help understand other less grid-like graph data, which we will discuss later.</p>
<h3 id="images-as-graphs">Images as graphs</h3>
<p>We typically think of images as rectangular grids with image channels, representing them as arrays (e.g., 244x244x3 floats). Another way to think of images is as graphs with regular structure, where each pixel represents a node and is connected via an edge to adjacent pixels. Each non-border pixel has exactly 8 neighbors, and the information stored at each node is a 3-dimensional vector representing the RGB value of the pixel.</p>
<p>A way of visualizing the connectivity of a graph is through its <em>adjacency matrix</em>. We order the nodes, in this case each of 25 pixels in a simple 5x5 image of a smiley face, and fill a matrix of $n_{nodes} \times n_{nodes}$ with an entry if two nodes share an edge. Note that each of these three representations below are different views of the same piece of data. </p>
<figure class="fullscreen">
<div id="image-as-graph" style="padding-bottom: 10px;"></div>
<figcaption>
Click on an image pixel to toggle its value, and see how the graph representation changes.
</figcaption>
</figure>

<h3 id="text-as-graphs">Text as graphs</h3>
<p>We can digitize text by associating indices to each character, word, or token, and representing text as a sequence of these indices. This creates a simple directed graph, where each character or index is a node and is connected via an edge to the node that follows it.</p>
<figure>
<div id="text-as-graph"></div>
<figcaption>
Edit the text above to see how the graph representation changes.
</figcaption>
</figure>

<p>Of course, in practice, this is not usually how text and images are encoded: these graph representations are redundant since all images and all text will have very regular structures. For instance, images have a banded structure in their adjacency matrix because all nodes (pixels) are connected in a grid. The adjacency matrix for text is just a diagonal line, because each word only connects to the prior word, and to the next one. </p>
<aside markdown="1">
This representation (a sequence of character tokens) refers to the way text is often represented in RNNs; other models, such as Transformers, can be considered to view text as a fully connected graph where we learn the relationship between tokens. See more in <a href="#graph-attention-networks">Graph Attention Networks</a>.
</aside>

<h3 id="graph-valued-data-in-the-wild">Graph-valued data in the wild</h3>
<p>Graphs are a useful tool to describe data you might already be familiar with. Letâ€™s move on to data which is more heterogeneously structured. In these examples, the number of neighbors to each node is variable (as opposed to the fixed neighborhood size of images and text). This data is hard to phrase in any other way besides a graph.</p>
<p><strong>Molecules as graphs.</strong> Molecules are the building blocks of matter, and are built of atoms and electrons in 3D space. All particles are interacting, but when a pair of atoms are stuck in a stable distance from each other, we say they share a covalent bond. Different pairs of atoms and bonds have different distances (e.g. single-bonds, double-bonds). Itâ€™s a very convenient and common abstraction to describe this 3D object as a graph, where nodes are atoms and edges are covalent bonds. <d-cite key="Duvenaud2015-yc"></d-cite> Here are two common molecules, and their associated graphs.</p>
<figure class="fullscreen"><div id="mols-as-graph-citronellal"></div>
<figcaption>
(Left) 3d representation of the Citronellal molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.
</figcaption>
</figure>

<figure class="fullscreen"><div id="mols-as-graph-caffeine"></div>
<figcaption>
(Left) 3d representation of the Caffeine molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.
</figcaption>
</figure>


<p><strong>Social networks as graphs.</strong> Social networks are tools to study patterns in collective behaviour of people, institutions and organizations. We can build a graph representing groups of people by modelling individuals as nodes, and their relationships as edges. </p>
<figure class="fullscreen"><div id="mols-as-graph-othello"></div>
<figcaption>
(Left) Image of a scene from the play â€œOthelloâ€. (Center) Adjacency matrix of the interaction between characters in the play. (Right) Graph representation of these interactions.
</figcaption>
</figure>

<p>Unlike image and text data, social networks do not have identical adjacency matrices. </p>
<figure class="fullscreen"><div id="mols-as-graph-karate"></div>
<figcaption>
(Left) Image of karate tournament. (Center) Adjacency matrix of the interaction between people in a karate club. (Right) Graph representation of these interactions.
</figcaption>
</figure>

<p><strong>Citation networks as graphs.</strong> Scientists routinely cite other scientistsâ€™ work when publishing papers. We can visualize these networks of citations as a graph, where each paper is a node, and each <em>directed</em> edge is a citation between one paper and another. Additionally, we can add information about each paper into each node, such as a word embedding of the abstract. (see <d-cite key="Mikolov2013-vr"></d-cite>,  <d-cite key="Devlin2018-mi"></d-cite>&nbsp;,  <d-cite key="Pennington2014-kg"></d-cite>). </p>
<p><strong>Other examples.</strong> In computer vision, we sometimes want to tag objects in visual scenes. We can then build graphs by treating these objects as nodes, and their relationships as edges. <a href="https://www.tensorflow.org/tensorboard/graphs">Machine learning models</a>, <a href="https://openreview.net/pdf?id=BJOFETxR-">programming code</a> <d-cite key="Allamanis2017-kz"></d-cite> and <a href="https://openreview.net/forum?id=S1eZYeHFDS">math equations</a><d-cite key="Lample2019-jg"></d-cite> can also be phrased as graphs, where the variables are nodes, and edges are operations that have these variables as input and output. You might see the term â€œdataflow graphâ€ used in some of these contexts.</p>
<p>The structure of real-world graphs can vary greatly between different types of dataâ€‰â€”â€‰some graphs have many nodes with few connections between them, or vice versa. Graph datasets can vary widely (both within a given dataset, and between datasets) in terms of the number of nodes, edges, and the connectivity of nodes.</p>
<figure>
<div id="table"></div>
<figcaption>

<p>Summary statistics on graphs found in the real world. Numbers are dependent on featurization decisions. More useful statistics and graphs can be found in KONECT<d-cite key="Kunegis2013-er"></d-cite></p>
</figcaption></figure>

<h2 id="what-types-of-problems-have-graph-structured-data">What types of problems have graph structured data?</h2>
<p>We have described some examples of graphs in the wild, but what tasks do we want to perform on this data? There are three general types of prediction tasks on graphs: graph-level, node-level, and edge-level. </p>
<p>In a graph-level task, we predict a single property for a whole graph. For a node-level task, we predict some property for each node in a graph. For an edge-level task, we want to predict the property or presence of edges in a graph.</p>
<p>For the three levels of prediction problems described above (graph-level, node-level, and edge-level), we will show that all of the following problems can be solved with a single model class, the GNN. But first, letâ€™s take a tour through the three classes of graph prediction problems in more detail, and provide concrete examples of each.</p>
<aside>
There are other related tasks that are areas of active research. For instance, we might want to <a href="#generative-modelling">generate graphs</a>, or <a href="#graph-explanations-and-attributions">explain predictions on a graph</a>. More topics can be found in the <a href="#into-the-weeds">Into the weeds section </a>.
</aside>

<h3 id="graph-level-task">Graph-level task</h3>
<p>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.</p>
<figure>
<div id="graph-level-problems"></div>
</figure>

<p>This is analogous to image classification problems with MNIST and CIFAR, where we want to associate a label to an entire image. With text, a similar problem is sentiment analysis where we want to identify the mood or emotion of an entire sentence at once.</p>
<h3 id="node-level-task">Node-level task</h3>
<p>Node-level tasks are concerned with predicting the identity or role of each node within a graph.</p>
<p>A classic example of a node-level prediction problem is Zachâ€™s karate club.<d-cite key="Zachary1977-jg"></d-cite> The dataset is a single social network graph made up of individuals that have sworn allegiance to one of two karate clubs after a political rift. As the story goes, a feud between Mr. Hi (Instructor) and John H (Administrator) creates a schism in the karate club. The nodes represent individual karate practitioners, and the edges represent interactions between these members outside of karate. The prediction problem is to classify whether a given member becomes loyal to either Mr. Hi or John H, after the feud. In this case, distance between a node to either the Instructor or Administrator is highly correlated to this label.</p>
<figure>
<div id="node-level-problems"></div>
<figcaption>
On the left we have the initial conditions of the problem, on the right we have a possible solution, where each node has been classified based on the alliance. The dataset can be used in other graph problems like unsupervised learning. 
</figcaption></figure>

<p>Following the image analogy, node-level prediction problems are analogous to <em>image segmentation</em>, where we are trying to label the role of each pixel in an image. With text, a similar task would be predicting the parts-of-speech of each word in a sentence (e.g. noun, verb, adverb, etc).</p>
<h3 id="edge-level-task">Edge-level task</h3>
<p>The remaining prediction problem in graphs is <em>edge prediction</em>. </p>
<p>One example of edge-level inference is in image scene understanding. Beyond identifying objects in an image, deep learning models can be used to predict the relationship between them. We can phrase this as an edge-level classification: given nodes that represent the objects in the image, we wish to predict which of these nodes share an edge or what the value of that edge is. If we wish to discover connections between entities, we could consider the graph fully connected and based on their predicted value prune edges to arrive at a sparse graph.</p>
<figure>
<img src="merged.0084f617.png" '="">
<figcaption>
In (b), above, the original image (a) has been segmented into five entities: each of the fighters, the referee, the audience and the mat. (C) shows the relationships between these entities. 
</figcaption></figure>

<figure>
<img src="edges_level_diagram.c40677db.png" '="">
<figcaption>
On the left we have an initial graph built from the previous visual scene. On the right is a possible edge-labeling of this graph when some connections were pruned based on the modelâ€™s output.
</figcaption></figure>

<h2 id="the-challenges-of-using-graphs-in-machine-learning">The challenges of using graphs in machine learning</h2>
<p>So, how do we go about solving these different graph tasks with neural networks? The first step is to think about how we will represent graphs to be compatible with neural networks.</p>
<p>Machine learning models typically take rectangular or grid-like arrays as input. So, itâ€™s not immediately intuitive how to represent them in a format that is compatible with deep learning. Graphs have up to four types of information that we will potentially want to use to make predictions: nodes, edges,  global-context and connectivity. The first three are relatively straightforward: for example, with nodes we can form a node feature matrix $N$ by assigning each node an index $i$ and storing the feature for $node_i$ in $N$. While these matrices have a variable number of examples, they can be processed without any special techniques.</p>
<p>However, representing a graphâ€™s connectivity is more complicated. Perhaps the most obvious choice would be to use an adjacency matrix, since this is easily tensorisable. However, this representation has a few drawbacks. From the <a href="#table">example dataset table</a>, we see the number of nodes in a graph can be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient.</p>
<p>Another problem is that there are many adjacency matrices that can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep neural network (that is to say, they are not permutation invariant).</p>
<aside markdown="1">
Learning permutation invariant operations is an area of recent research.<d-cite key="Mena2018-ce"></d-cite><d-cite key="Murphy2018-fz"></d-cite>
</aside>

<p>For example, the <a href="mols-as-graph-othello"> Othello graph </a> from before can be described equivalently with these two adjacency matrices.  It can also be described with every other possible permutation of the nodes.</p>
<figure>
<div class="two-imgs">
<img src="othello1.246371ea.png">
<img src="othello2.6897c848.png">
</div>
<figcaption>
Two adjacency matrices representing the same graph.
</figcaption>
</figure>

<p>The example below shows every adjacency matrix that can describe this small graph of 4 nodes. This is already a significant number of adjacency matricesâ€“for larger examples like Othello, the number is untenable.</p>
<figure class="fullscreen"><div id="shuffle-sm"></div>
<figcaption>
All of these adjacency matrices represent the same graph. Click on an edge to remove it on a â€œvirtual edgeâ€ to add it and the matrices will update accordingly.
</figcaption>
</figure>

<p>One elegant and memory-efficient way of representing sparse matrices is as adjacency lists. These describe the connectivity of edge $e_k$ between nodes $n_i$ and $n_j$ as a tuple (i,j) in the k-th entry of an adjacency list. Since we expect the number of edges to be much lower than the number of entries for an adjacency matrix ($n_{nodes}^2$), we avoid computation and storage on the disconnected parts of the graph. </p>
<aside>
Another way of stating this is with Big-O notation, it is preferable to have $O(n_{edges})$, rather than $O(n_{nodes}^2)$.
</aside>

<p>To make this notion concrete, we can see how information in different graphs might be represented under this specification:</p>
<figure class="fullscreen">
<div id="graph-to-tensor"></div>
<figcaption>
Hover and click on the edges, nodes, and global graph marker to view and change attribute representations. On one side we have a small graph and on the other the information of the graph in a tensor representation.
</figcaption></figure>

<p>It should be noted that the figure uses scalar values per node/edge/global, but most practical tensor representations have vectors per graph attribute. Instead of a node tensor of size $[n_{nodes}]$ we will be dealing with node tensors of size $[n_{nodes}, node_{dim}]$. Same for the other graph attributes.</p>
<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>Now that the  graphâ€™s description is in a matrix format that is permutation invariant, we will describe using graph neural networks (GNNs) to solve graph prediction tasks. <strong>A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).</strong> Weâ€™re going to build GNNs using the â€œmessage passing neural networkâ€ framework proposed by Gilmer et al.<d-cite key="Gilmer2017-no"></d-cite> using the Graph Nets architecture schematics introduced by Battaglia et al.<d-cite key="Battaglia2018-pi"></d-cite>  GNNs adopt a â€œgraph-in, graph-outâ€ architecture meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph. </p>
<h3 id="the-simplest-gnn">The simplest GNN</h3>
<p>With the numerical representation of graphs that <a href="#graph-to-tensor">weâ€™ve constructed above</a> (with vectors instead of scalars), we are now ready to build a GNN. We will start with the simplest GNN architecture, one where we learn new embeddings for all graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph.</p>
<aside>
For simplicity, the previous diagrams used scalars to represent graph attributes; in practice feature vectors, or embeddings, are much more useful.   
</aside>

<p>This GNN uses a separate multilayer perceptron (MLP) (or your favorite differentiable model) on each component of a graph; we call this a GNN layer. For each node vector, we apply the MLP and get back a learned node-vector. We do the same for each edge, learning a per-edge embedding, and also for the global-context vector, learning a single embedding for the entire graph.</p>
<aside>
You could also call it a GNN block. Because it contains multiple operations/layers (like a ResNet block).
</aside>

<figure>
<img src="arch_independent.0efb8ae7.png" '="">
<figcaption>
A single layer of a simple GNN. A graph is the input, and each component (V,E,U) gets updated by a MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of a GNN model.
</figcaption></figure>

<p>As is common with  neural networks modules or layers, we can stack these GNN layers together. </p>
<p>Because a GNN does not update the connectivity of the input graph, we can describe the output graph of a GNN with the same adjacency list and the same number of feature vectors as the input graph. But, the output graph has updated embeddings, since the GNN has updated each of the node, edge and global-context representations.</p>
<h3 id="gnn-predictions-by-pooling-information">GNN Predictions by Pooling Information</h3>
<p>We have built a simple GNN, but how do we make predictions in any of the tasks we described above?</p>
<p>We will consider the case of binary classification, but this framework can easily be extended to the multi-class or regression case. If the task is to make binary predictions on nodes, and the graph already contains node information, the approach is straightforwardâ€‰â€”â€‰for each node embedding, apply a linear classifier.</p>
<figure><img src="prediction_nodes_nodes.c2c8b4d0.png" '=""></figure>

<aside markdown="1">
We could imagine a social network, where we wish to anonymize user data (nodes) by not using them, and only using relational data (edges). One instance of such a scenario is the node task we specified in the <a href="#node-level-task"> Node-level task</a> subsection. In the Karate club example, this would be just using the number of meetings between people to determine the alliance to Mr. Hi or John H.
</aside>

<p>However, it is not always so simple. For instance, you might have information in the graph stored in edges, but no information in nodes, but still need to make predictions on nodes. We need a way to collect information from edges and give them to nodes for prediction. We can do this by <em>pooling</em>. Pooling proceeds in two steps:</p>
<ol>
<li><p>For each item to be pooled, <em>gather</em> each of their embeddings and concatenate them into a matrix.</p>
</li>
<li><p>The gathered embeddings are then <em>aggregated</em>, usually via a sum operation.</p>
</li>
</ol>
<aside markdown="1">
For a more in-depth discussion on aggregation operations go to the <a href="#comparing-aggregation-operations"> Comparing aggregation operations </a> section.
</aside>

<p>We represent the <em>pooling</em> operation by the letter $\rho$, and denote that we are gathering information from edges to nodes as $p_{E_n \to V_{n}}$. </p>
<figure><div id="node-step-small"></div>
<figcaption>
Hover over a node (black node) to visualize which edges are gathered and aggregated to produce an embedding for that target node.</figcaption>
</figure>


<p>So If we only have edge-level features, and are trying to predict binary node information, we can use pooling to route (or pass) information to where it needs to go. The model looks like this. </p>
<figure><img src="prediction_edges_nodes.e6796b8e.png" style="padding-bottom: 10px;" '="">
<figcaption>
</figcaption>
</figure>

<p>If we only have node-level features, and are trying to predict binary edge-level information, the model looks like this.</p>
<figure><img src="prediction_nodes_edges.26fadbcc.png" '=""></figure>

<aside markdown="1">
One example of such a scenario is the edge task we specified in <a href="#edge-level-task"> Edge level task</a> sub section. Nodes can be recognized as image entities, and we are trying to predict if the entities share a relationship (binary edges).
</aside>

<p>If we only have node-level features, and need to predict a binary global property, we need to gather all available node information together and aggregate them. This is similar to <em>Global Average Pooling</em> layers in CNNs. The same can be done for edges.</p>
<figure><img src="prediction_nodes_edges_global.7a535eb8.png" '=""></figure>

<aside markdown="1">
This is a common scenario for predicting molecular properties. For example, we have atomic information, connectivity and we would like to know the toxicity of a molecule (toxic/not toxic), or if it has a particular odor (rose/not rose).
</aside>

<p>In our examples, the classification model <em>$c$</em> can easily be replaced with any differentiable model, or adapted to multi-class classification using a generalized linear model.</p>
<figure><img src="Overall.e3af58ab.png" '="">
<figcaption>
An end-to-end prediction task with a GNN model.
</figcaption>
</figure>

<p>Now weâ€™ve demonstrated that we can build a simple GNN model, and make binary predictions by routing information between different parts of the graph. This pooling technique will serve as a building block for constructing more sophisticated GNN models. If we have new graph attributes, we just have to define how to pass information from one attribute to another. </p>
<p>Note that in this simplest GNN formulation, weâ€™re not using the connectivity of the graph at all inside the GNN layer. Each node is processed independently, as is each edge, as well as the global context. We only use connectivity when pooling information for prediction. </p>
<h3 id="passing-messages-between-parts-of-the-graph">Passing messages between parts of the graph</h3>
<p>We could make more sophisticated predictions by using pooling within the GNN layer, in order to make our learned embeddings aware of graph connectivity. We can do this using <em>message passing</em><d-cite key="Gilmer2017-no"></d-cite>, where neighboring nodes or edges exchange information and influence each otherâ€™s updated embeddings.</p>
<p>Message passing works in three steps: </p>
<ol>
<li><p>For each node in the graph, <em>gather</em> all the neighboring node embeddings (or messages), which is the $g$ function described above.</p>
</li>
<li><p>Aggregate all messages via an aggregate function (like sum).</p>
</li>
<li><p>All pooled messages are passed through an <em>update function</em>, usually a learned neural network.</p>
</li>
</ol>
<aside>
You could also 1) gather messages, 3) update them and 2) aggregate them and still have a permutation invariant operation.<d-cite key="Zaheer2017-uc"></d-cite> 
</aside>

<p>Just as pooling can be applied to either nodes or edges, message passing can occur between either nodes or edges.</p>
<p>These steps are key for leveraging the connectivity of graphs. We will build more elaborate variants of message passing in GNN layers that yield GNN models of increasing expressiveness and power. </p>
<figure class="fullscreen"><div id="node-step" style="padding-bottom: 10px;"></div>
<figcaption>
Hover over a node, to highlight adjacent nodes and visualize the adjacent embedding that would be pooled, updated and stored.
</figcaption>
</figure>

<p>This sequence of operations, when applied once, is the simplest type of message-passing GNN layer.</p>
<p>This is reminiscent of standard convolution: in essence, message passing and convolution are operations to aggregate and process the information of an elementâ€™s neighbors in order to update the elementâ€™s value. In graphs, the element is a node, and in images, the element is a pixel. However, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.</p>
<p>By stacking message passing GNN layers together, a node can eventually incorporate information from across the entire graph: after three layers, a node has information about the nodes three steps away from it.</p>
<p>We can update our architecture diagram to include this new source of information for nodes:</p>
<figure><img src="arch_gcn.40871750.png" '="">
<figcaption>
Schematic for a GCN architecture, which updates node representations of a graph by pooling neighboring nodes at a distance of one degree.
</figcaption></figure>

<h3 id="learning-edge-representations">Learning edge representations</h3>
<p>Our dataset does not always contain all types of information (node, edge, and global context). 
When we want to make a prediction on nodes, but our dataset only has edge information, we showed above how to use pooling to route information from edges to nodes, but only at the final prediction step of the model. We can share information between nodes and edges within the GNN layer using message passing.</p>
<p>We can incorporate the information from neighboring edges in the same way we used neighboring node information earlier, by first pooling the edge information, transforming it with an update function, and storing it.</p>
<p>However, the node and edge information stored in a graph are not necessarily the same size or shape, so it is not immediately clear how to combine them. One way is to learn a linear mapping from the space of edges to the space of nodes, and vice versa. Alternatively, one may concatenate them together before the update function.</p>
<figure><img src="arch_mpnn.a13c2294.png">
<figcaption>
Architecture schematic for Message Passing layer. The first step â€œpreparesâ€ a message composed of information from an edge and itâ€™s connected nodes and then â€œpassesâ€ the message to the node.
</figcaption></figure>

<p>Which graph attributes we update and in which order we update them is one design decision when constructing GNNs. We could choose whether to update node embeddings before edge embeddings, or the other way around. This is an open area of research with a variety of solutionsâ€“ for example we could update in a â€˜weaveâ€™ fashion<d-cite key="Kearnes2016-rl"></d-cite> where we have four updated representations that get combined into new node and edge representations: node to node (linear), edge to edge (linear), node to edge (edge layer), edge to node (node layer).</p>
<figure><img src="arch_weave.352befc0.png">
<figcaption>
Some of the different ways we might combine edge and node representation in a GNN layer.
</figcaption></figure>

<h3 id="adding-global-representations">Adding global representations</h3>
<p>There is one flaw with the networks we have described so far: nodes that are far away from each other in the graph may never be able to efficiently transfer information to one another, even if we apply message passing several times. For one node, If we have k-layers, information will propagate at most k-steps away.  This can be a problem for situations where the prediction task depends on nodes, or groups of nodes, that are far apart.  One solution would be to have all nodes be able to pass information to each other. 
Unfortunately for large graphs, this quickly becomes computationally expensive (although this approach, called â€˜virtual edgesâ€™, has been used for small graphs such as molecules).<d-cite key="Gilmer2017-no"></d-cite></p>
<p>One solution to this problem is by using the global representation of a graph (U) which is sometimes called a <strong>master node</strong> <d-cite key="Battaglia2018-pi"></d-cite><d-cite key="Gilmer2017-no"></d-cite> or context vector. This global context vector is connected to all other nodes and edges in the network, and can act as a bridge between them to pass information, building up a representation for the graph as a whole. This creates a richer and more complex representation of the graph than could have otherwise been learned. </p>
<figure><img src="arch_graphnet.b229be6d.png">
<figcaption>Schematic of a Graph Nets architecture leveraging global representations.
</figcaption></figure>

<p>In this view all graph attributes have learned representations, so we can leverage them during pooling by conditioning the information of our attribute of interest with respect to the rest. For example, for one node we can consider information from neighboring nodes, connected edges and the global information. To condition the new node embedding on all these possible sources of information, we can simply concatenate them. Additionally we may also map them to the same space via a linear map and add them or apply a feature-wise modulation layer<d-cite key="Dumoulin2018-tb"></d-cite>, which can be considered a type of featurize-wise attention mechanism.</p>
<figure><img src="graph_conditioning.3017e214.png">
<figcaption>Schematic for conditioning the information of one node based on three other embeddings (adjacent nodes, adjacent edges, global). This step corresponds to the node operations in the Graph Nets Layer. 
</figcaption></figure>

<h2 id="gnn-playground">GNN playground</h2>
<p>Weâ€™ve described a wide range of GNN components here, but how do they actually differ in practice? This GNN playground allows you to see how these different components and architectures contribute to a GNNâ€™s ability to learn a real task. </p>
<p>Our playground shows a graph-level prediction task with small molecular graphs. We use the the Leffingwell Odor Dataset<d-cite key="Sanchez-Lengeling2020-qq"></d-cite><d-cite key="Sanchez-Lengeling2019-vs"></d-cite>, which is composed of molecules with associated odor percepts (labels). Predicting the relation of a molecular structure (graph) to its smell is a 100 year-old problem straddling chemistry, physics, neuroscience, and machine learning.</p>
<p>To simplify the problem,  we consider only a single binary label per molecule, classifying if a molecular graph smells â€œpungentâ€ or not, as labeled by a professional perfumer. We say a molecule has a â€œpungentâ€ scent if it has a strong, striking smell. For example, garlic and mustard, which might contain the molecule <em>allyl alcohol</em> have this quality. The molecule <em>piperitone</em>, often used for peppermint-flavored candy, is also described as having a pungent smell.</p>
<p>We represent each molecule as a graph, where atoms are nodes containing a one-hot encoding for its atomic identity (Carbon, Nitrogen, Oxygen, Fluorine) and bonds are edges containing a one-hot encoding its bond type (single, double, triple or aromatic). </p>
<p>Our general modeling template for this problem will be built up using sequential GNN layers, followed by a linear model with a sigmoid activation for classification. The design space for our GNN has many levers that can customize the model:</p>
<ol>
<li><p>The number of GNN layers, also called the <em>depth</em>.</p>
</li>
<li><p>The dimensionality of each attribute when updated. The update function is a 1-layer MLP with a relu activation function and a layer norm for normalization of activations. </p>
</li>
<li><p>The aggregation function used in pooling: max, mean or sum.</p>
</li>
<li><p>The graph attributes that get updated, or styles of message passing: nodes, edges and global representation. We control these via boolean toggles (on or off). A baseline model would be a graph-independent GNN (all message-passing off) which aggregates all data at the end into a single global attribute. Toggling on all message-passing functions yields a GraphNets architecture.</p>
</li>
</ol>
<p>To better understand how a GNN is learning a task-optimized representation of a graph, we also look at the penultimate layer activations of the GNN. These â€˜graph embeddingsâ€™ are the outputs of the GNN model right before prediction.  Since we are using a generalized linear model for prediction, a linear mapping is enough to allow us to see how we are learning representations around the decision boundary. </p>
<p>Since these are high dimensional vectors, we reduce them to 2D via principal component analysis (PCA). 
A perfect model would visibility separate labeled data, but since we are reducing dimensionality and also have imperfect models, this boundary might be harder to see.</p>
<p>Play around with different model architectures to build your intuition. For example, see if you can edit the molecule on the left to make the model prediction increase. Do the same edits have the same effects for different model architectures?</p>
<aside>This playground is running live on the browser in <a href="https://www.tensorflow.org/js/">tfjs</a>.</aside>

<figure class="fullscreen"><div id="playground"></div>
<figcaption>Edit the molecule to see how the prediction changes, or change the model params to load a different model. Select a different molecule in the scatter plot.</figcaption></figure>


<h3 id="some-empirical-gnn-design-lessons">Some empirical GNN design lessons</h3>
<p>When exploring the architecture choices above, you might have found some models have better performance than others. Are there some clear GNN design choices that will give us better performance? For example, do deeper GNN models perform better than shallower ones? or is there a clear choice between aggregation functions? The answers are going to depend on the data, <d-cite key="Dwivedi2020-xm"></d-cite> <d-cite key="You2020-vk"></d-cite>, and even different ways of featurizing and constructing graphs can give different answers.</p>
<p>With the following interactive figure, we explore the space of GNN architectures and the performance of this task across a few major design choices:  Style of message passing, the dimensionality of embeddings, number of layers, and aggregation operation type.</p>
<p>Each point in the scatter plot represents a model: the x axis is the number of trainable variables, and the y axis is the performance. Hover over a point to see the GNN architecture parameters.</p>
<figure>
<div id="BasicArchitectures"></div>
<script>var spec = "BasicArchitectures.json";
vegaEmbed('#BasicArchitectures', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
}).catch(console.error);</script>
<figcaption>Scatterplot of each modelâ€™s performance vs its number of trainable variables. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>

<p>The first thing to notice is that, surprisingly, a higher number of parameters does correlate with higher performance. GNNs are a very parameter-efficient model type: for even a small number of parameters (3k) we can already find models with high performance. </p>
<p>Next, we can look at the distributions of performance aggregated based on the dimensionality of the learned representations for different graph attributes.</p>
<figure>
<div id="ArchitectureNDim"></div>
<script>var spec = "ArchitectureNDim.json";
vegaEmbed('#ArchitectureNDim', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
}).catch(console.error);</script>
<figcaption>Aggregate performance of models across varying node, edge, and global dimensions.</figcaption>
</figure>

<p>We can notice that models with higher dimensionality tend to have better mean and lower bound performance but the same trend is not found for the maximum. Some of the top-performing models can be found for smaller dimensions. Since higher dimensionality is going to also involve a higher number of parameters, these observations go in hand with the previous figure.</p>
<p>Next we can see the breakdown of performance based on the number of GNN layers.</p>
<figure>
<div id="ArchitectureNLayers"></div>
<script>var spec = "ArchitectureNLayers.json";
vegaEmbed('#ArchitectureNLayers', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
}).catch(console.error);</script>
<figcaption> Chart of number of layers vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by the number of layers. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>


<p>The box plot shows a similar trend, while the mean performance tends to increase with the number of layers, the best performing models do not have three or four layers, but two. Furthermore, the lower bound for performance decreases with four layers. This effect has been observed before, GNN with a higher number of layers will broadcast information at a higher distance and can risk having their node representations â€˜dilutedâ€™ from many successive iterations <d-cite key="Corso2020-py"></d-cite>.</p>
<p>Does our dataset have a preferred aggregation operation? Our following figure breaks down performance in terms of aggregation type.</p>
<figure>
<div id="ArchitectureAggregation"></div>
<script>var spec = "ArchitectureAggregation.json";
vegaEmbed('#ArchitectureAggregation', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
}).catch(console.error);</script>
<figcaption>Chart of aggregation type vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by aggregation type. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>

<p>Overall it appears that sum has a very slight improvement on the mean performance, but max or mean can give equally good models. This is useful to contextualize when looking at the <a href="#comparing-aggregation-operations"> discriminatory/expressive capabilities</a> of aggregation operations&nbsp;.</p>
<p>The previous explorations have given mixed messages. We can find mean trends where more complexity gives better performance but we can find clear counterexamples where models with fewer parameters, number of layers, or dimensionality perform better. One trend that is much clearer is about the number of attributes that are passing information to each other.</p>
<p>Here we break down performance based on the style of message passing. On both extremes, we consider models that do not communicate between graph entities (â€œnoneâ€) and models that have messaging passed between nodes, edges, and globals.</p>
<figure>
<div id="ArchitectureMessagePassing"></div>
<script>var spec = "ArchitectureMessagePassing.json";
vegaEmbed('#ArchitectureMessagePassing', spec).then(function (result) {// Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
}).catch(console.error);</script>
<figcaption>Chart of message passing vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by message passing. Hover over a point to see the GNN architecture parameters</figcaption>
</figure>

<p>Overall we see that the more graph attributes are communicating, the better the performance of the average model. Our task is centered on global representations, so explicitly learning this attribute also tends to improve performance. Our node representations also seem to be more useful than edge representations, which makes sense since more information is loaded in these attributes.</p>
<p>There are many directions you could go from here to get better performance. We wish two highlight two general directions, one related to more sophisticated graph algorithms and another towards the graph itself.</p>
<p>Up until now, our GNN is based on a neighborhood-based pooling operation. There are some graph concepts that are harder to express in this way, for example a linear graph path (a connected chain of nodes). Designing new mechanisms in which graph information can be extracted, executed and propagated in a GNN is one current research area <d-cite key="Markowitz2021-rn"></d-cite>, <d-cite key="Du2019-hr"></d-cite>, <d-cite key="Xu2018-hq"></d-cite>, <d-cite key="Velickovic2019-io"></d-cite>.</p>
<p>One of the frontiers of GNN research is not making new models and architectures, but â€œhow to construct  graphsâ€, to be more precise, imbuing graphs with additional structure or relations that can be leveraged. As we loosely saw, the more graph attributes are communicating the more we tend to have better models. In this particular case, we could consider making molecular graphs more feature rich, by adding additional spatial relationships between nodes, adding edges that are not bonds, or explicit learnable relationships between subgraphs.</p>
<aside>See more in <a href="#Other-types-of-graphs ">Other types of graphs</a>.</aside>

<h2 id="into-the-weeds">Into the Weeds</h2>
<p>Next, we have a few sections on a myriad of graph-related topics that are relevant for GNNs.</p>
<h3 id="other-types-of-graphs-multigraphs-hypergraphs-hypernodes-hierarchical-graphs">Other types of graphs (multigraphs, hypergraphs, hypernodes, hierarchical graphs)</h3>
<p>While we only described graphs with vectorized information for each attribute, graph structures are more flexible and can accommodate other types of information. Fortunately, the message passing framework is flexible enough that often adapting GNNs to more complex graph structures is about defining how information is passed and updated by new graph attributes. </p>
<p>For example, we can consider multi-edge graphs or <em>multigraphs</em><d-cite key="Harary1969-qo"></d-cite>, where a pair of nodes can share multiple types of edges, this happens when we want to model the interactions between nodes differently based on their type. For example with a social network, we can specify edge types based on the type of relationships (acquaintance, friend, family). A GNN can be adapted by having different types of message passing steps for each edge type. 
We can also consider nested graphs, where for example a node represents a graph, also called a hypernode graph.<d-cite key="Poulovassilis1994-bt"></d-cite> Nested graphs are useful for representing hierarchical information. For example, we can consider a network of molecules, where a node represents a molecule and an edge is shared between two molecules if we have a way (reaction) of transforming one to the other <d-cite key="Zitnik2018-uk"></d-cite>  <d-cite key="Stocker2020-tr"></d-cite>.
In this case, we can learn on a nested graph by having a GNN that learns representations at the molecule level and another at the reaction network level, and alternate between them during training.</p>
<p>Another type of graph is a hypergraph<d-cite key="Berge1976-ss"></d-cite>, where an edge can be connected to multiple nodes instead of just two. For a given graph, we can build a hypergraph by identifying communities of nodes and assigning a hyper-edge that is connected to all nodes in a community.</p>
<figure><img src="multigraphs.1bb84306.png">
<figcaption>Schematic of more complex graphs. On the left we have an example of a multigraph with three edge types, including a directed edge. On the right we have a three-level hierarchical graph, the intermediate level nodes are hypernodes.
</figcaption></figure>

<p>How to train and design GNNs that have multiple types of graph attributes is a current area of research <d-cite key="Yadati2018-de"></d-cite>, <d-cite key="Zhong2020-mv"></d-cite>.</p>
<h3 id="sampling-graphs-and-batching-in-gnns">Sampling Graphs and Batching in GNNs</h3>
<p>A common practice for training neural networks is to update network parameters with gradients calculated on randomized constant size (batch size) subsets of the training data (mini-batches). This practice presents a challenge for graphs due to the variability in the number of nodes and edges adjacent to each other, meaning that we cannot have a constant batch size. The main idea for batching with graphs is to create subgraphs that preserve essential properties of the larger graph. This graph sampling operation is highly dependent on context and involves sub-selecting nodes and edges from a graph. These operations might make sense in some contexts (citation networks) and in others, these might be too strong of an operation (molecules, where a subgraph simply represents a new, smaller molecule). How to sample a graph is an open research question.<d-cite key="Rozemberczki2020-lq"></d-cite> 
If we care about preserving structure at a neighborhood level, one way would be to randomly sample a uniform number of nodes, our <em>node-set</em>. Then add neighboring nodes of distance k adjacent to the node-set, including their edges.<d-cite key="Leskovec2006-st"></d-cite> Each neighborhood can be considered an individual graph and a GNN can be trained on batches of these subgraphs. The loss can be masked to only consider the node-set since all neighboring nodes would have incomplete neighborhoods.
A more efficient strategy might be to first randomly sample a single node, expand its neighborhood to distance k, and then pick the other node within the expanded set. These operations can be terminated once a certain amount of nodes, edges, or subgraphs are constructed.
If the context allows, we can build constant size neighborhoods by picking an initial node-set and then sub-sampling a constant number of nodes (e.g randomly, or via a random walk or Metropolis algorithm<d-cite key="Hubler2008-us"></d-cite>).</p>
<figure><img src="sampling.968003b3.png">
<figcaption>Four different ways of sampling the same graph. Choice of sampling strategy depends highly on context since they will generate different distributions of graph statistics (# nodes, #edges, etc.). For highly connected graphs, edges can be also subsampled. 
</figcaption></figure>

<p>Sampling a graph is particularly relevant when a graph is large enough that it cannot be fit in memory. Inspiring new architectures and training strategies such as Cluster-GCN <d-cite key="Chiang2019-yh"></d-cite>  and GraphSaint <d-cite key="Zeng2019-eh"></d-cite>. We expect graph datasets to continue growing in size in the future.</p>
<h3 id="inductive-biases">Inductive biases</h3>
<p>When building a model to solve a problem on a specific kind of data, we want to specialize our models to leverage the characteristics of that data. When this is done successfully, we often see better predictive performance, lower training time, fewer parameters and better generalization.  </p>
<p>When labeling on images, for example, we want to take advantage of the fact that a dog is still a dog whether it is in the top-left or bottom-right corner of an image. Thus, most image models use convolutions, which are translation invariant. For text, the order of the tokens is highly important, so recurrent neural networks process data sequentially. Further, the presence of one token (e.g. the word â€˜notâ€™) can affect the meaning of the rest of a sentence, and so we need components that can â€˜attendâ€™ to other parts of the text, which transformer models like BERT and GPT-3 can do. These are some examples of inductive biases, where we are identifying symmetries or regularities in the data and adding modelling components that take advantage of these properties.</p>
<p>In the case of graphs, we care about how each graph component (edge, node, global) is related to each other so we seek models that have a relational inductive bias.<d-cite key="Battaglia2018-pi"></d-cite> A model should preserve explicit relationships between entities (adjacency matrix) and preserve graph symmetries (permutation invariance). We expect problems where the interaction between entities is important will benefit from a graph structure. Concretely, this means designing transformation on sets: the order of operation on nodes or edges should not matter and  the operation should work on a variable number of inputs. </p>
<h3 id="comparing-aggregation-operations">Comparing aggregation operations</h3>
<p>Pooling information from neighboring nodes and edges is a critical step in any reasonably powerful GNN architecture. Because each node has a variable number of neighbors, and because we want a differentiable method of aggregating this information, we want to use a smooth aggregation operation that is invariant to node ordering and the number of nodes provided.</p>
<p>Selecting and designing optimal aggregation operations is an open research topic.<d-cite key="Xu2018-sf"></d-cite> A desirable property of an aggregation operation is that similar inputs provide similar aggregated outputs, and vice-versa. Some very simple candidate permutation-invariant operations are sum, mean, and max. Summary statistics like variance also work. All of these take a variable number of inputs, and provide an output that is the same, no matter the input ordering. Letâ€™s explore the difference between these operations.</p>
<figure><div id="pooling-table"> </div>
<figcaption>
No pooling type can always distinguish between graph pairs such as max pooling on the left and sum / mean pooling on the right. 
</figcaption></figure>

<p>There is no operation that is uniformly the best choice. The mean operation can be useful when nodes have a highly-variable number of neighbors or you need a normalized view of the features of a local neighborhood. The max operation can be useful when you want to highlight single salient features in local neighborhoods. Sum provides a balance between these two, by providing a snapshot of the local distribution of features, but because it is not normalized, can also highlight outliers. In practice, sum is commonly used. </p>
<p>Designing aggregation operations is an open research problem that intersects with machine learning on sets.<d-cite key="Skianis2019-ds"></d-cite> New approaches such as Principal Neighborhood aggregation<d-cite key="Corso2020-py"></d-cite> take into account several aggregation operations by concatenating them and adding a scaling function that depends on the degree of connectivity of the entity to aggregate. Meanwhile, domain specific aggregation operations can also be designed. One example lies with the â€œTetrahedral Chiralityâ€ aggregation operators <d-cite key="Pattanaik2020-jj"></d-cite>.</p>
<h3 id="gcn-as-subgraph-function-approximators">GCN as subgraph function approximators</h3>
<p>Another way to see GCN (and MPNN) of k-layers with a 1-degree neighbor lookup is as a neural network that operates on learned embeddings of subgraphs of size k.<d-cite key="Liu2018-kf"></d-cite><d-cite key="Xu2018-sf"></d-cite></p>
<p>When focusing on one node, after k-layers, the updated node representation has a limited viewpoint of all neighbors up to k-distance, essentially a subgraph representation. Same is true for edge representations.</p>
<p>So a GCN is collecting all possible subgraphs of size k and learning vector representations from the vantage point of one node or edge. The number of possible subgraphs can grow combinatorially, so enumerating these subgraphs from the beginning vs building them dynamically as in a GCN, might be prohibitive.</p>
<figure><img src="arch_subgraphs.197f9b0e.png">
<figcaption>
</figcaption></figure>

<h3 id="edges-and-the-graph-dual">Edges and the Graph Dual</h3>
<p>One thing to note is that edge predictions and node predictions, while seemingly different, often reduce to the same problem: an edge prediction task on a graph $G$ can be phrased as a node-level prediction on $G$â€™s dual.</p>
<p>To obtain $G$â€™s dual, we can convert nodes to edges (and edges to nodes). A graph and its dual contain the same information, just expressed in a different way. Sometimes this property makes solving problems easier in one representation than another, like frequencies in Fourier space.  In short, to solve an edge classification problem on $G$, we can think about doing graph convolutions on $G$â€™s dual (which is the same as learning edge representations on $G$), this idea was developed with Dual-Primal Graph Convolutional Networks.<d-cite key="Monti2018-ov"></d-cite></p>
<!--[TODO: Image sketch of a graph and its dual]-->


<h3 id="graph-convolutions-as-matrix-multiplications-and-matrix-multiplications-as-walks-on-a-graph">Graph convolutions as matrix multiplications, and matrix multiplications as walks on a graph</h3>
<p>Weâ€™ve talked a lot about graph convolutions and message passing, and of course, this raises the question of how do we implement these operations in practice? For this section, we explore some of the properties of matrix multiplication, message passing, and its connection to traversing a graph. </p>
<p>The first point we want to illustrate is that the matrix multiplication of an adjacent matrix $A$  $n_{nodes} \times n_{nodes}$ with a node feature matrix $X$ of size $n_{nodes} \times node_{dim}$ implements an simple message passing with a summation aggregation.
Let the matrix be $B=AX$, we can observe that any entry $B_{ij}$ can be expressed as $&lt;A_{row_i} \dot X_{column_j}&gt;= A_{i,1}X_{1,j}+A_{i,2}X_{2, j}+â€¦+A_{i,n}X_{n, j}=\sum_{A_{i,k}&gt;0} X_{k,j}$. Because $A_{i,k}$ are binary entries only when a edge exists between $node_i$ and $node_k$, the inner product is essentially â€œgatheringâ€ all node features values of dimension $j$â€ that share an edge with $node_i$. It should be noted that this message passing is not updating the representation of the node features, just pooling neighboring node features. But this can be easily adapted by passing $X$ through your favorite differentiable transformation (e.g. MLP) before or after the matrix multiply.</p>
<p>From this view, we can appreciate the benefit of using adjacency lists. Due to the expected sparsity of $A$ we donâ€™t have to sum all values where $A_{i,j}$ is zero. As long as we have an operation to gather values based on an index, we should be able to just retrieve positive entries. Additionally, this matrix multiply-free approach frees us from using summation as an aggregation operation. </p>
<p>We can imagine that applying this operation multiple times allows us to propagate information at greater distances. In this sense, matrix multiplication is a form of traversing over a graph. This relationship is also apparent when we look at powers $A^K$ of the adjacency matrix.  If we consider the matrix $A^2$, the term $A^2_{ij}$ counts all walks of length 2 from $node_{i}$ to $node_{j}$ and can be expressed as the inner product $&lt;A_{row_i}, A_{column_j}&gt; = A_{i,1}A_{1, j}+A_{i,2}A_{2, j}+â€¦+A_{i,n}A{n,j}$. The intuition is that the first term $a_{i,1}a_{1, j}$ is only positive under two conditions, there is edge that connects $node_i$ to $node_1$ and another edge that connects $node_{1}$ to $node_{j}$. In other words, both edges form a path of length 2 that goes from $node_i$ to $node_j$ passing by $node_1$. Due to the summation, we are counting over all possible intermediate nodes. This intuition carries over when we consider $A^3=A \matrix A^2$.. and so on to $A^k$. </p>
<p>There are deeper connections on how we can view matrices as graphs to explore <d-cite key="noauthor_undated-qq"></d-cite><d-cite key="Bapat2014-fk"></d-cite><d-cite key="Bollobas2013-uk"></d-cite>.</p>
<h3 id="graph-attention-networks">Graph Attention Networks</h3>
<p>Another way of communicating information between graph attributes is via attention.<d-cite key="Vaswani2017-as"></d-cite> For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.The challenge then is to associate weights in a permutation invariant fashion. One approach is to consider a scalar scoring function that assigns weights based on pairs of nodes ( $f(node_i, node_j)$). In this case, the scoring function can be interpreted as a function that measures how relevant a neighboring node is in relation to the center node. Weights can be normalized, for example with a softmax function to focus most of the weight on a neighbor most relevant for a node in relation to a task. This concept is the basis of Graph Attention Networks (GAT) <d-cite key="Velickovic2017-hf"></d-cite> and Set Transformers<d-cite key="Lee2018-ti"></d-cite>. Permutation invariance is preserved, because scoring works on pairs of nodes. A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism. Additionally for interpretability, the scoring weights can be used as a measure of the importance of an edge in relation to a task. </p>
<figure><img src="attention.3c55769d.png">
<figcaption>Schematic of attention over one node with respect to itâ€™s adjacent nodes. For each edge an interaction score is computed, normalized and used to weight node embeddings.
</figcaption></figure>

<p>Additionally, transformers can be viewed as GNNs with an attention mechanism <d-cite key="Joshi2020-ze"></d-cite>. Under this view, the transformer models several elements (i.g. character tokens) as nodes in a fully connected graph and the attention mechanism is assigning edge embeddings to each node-pair which are used to compute attention weights. The difference lies in the assumed pattern of connectivity between entities, a GNN is assuming a sparse pattern and the Transformer is modelling all connections.</p>
<h3 id="graph-explanations-and-attributions">Graph explanations and attributions</h3>
<p>When deploying GNN in the wild we might care about model interpretability for building credibility, debugging or scientific discovery. The graph concepts that we care to explain vary from context to context. For example, with molecules we might care about the presence or absence of particular subgraphs<d-cite key="McCloskey2018-ml"></d-cite>, while in a citation network we might care about the degree of connectedness of an article. Due to the variety of graph concepts, there are many ways to build explanations. GNNExplainer<d-cite key="Ying2019-gk"></d-cite> casts this problem as extracting the most relevant subgraph that is important for a task. Attribution techniques<d-cite key="Pope2019-py"></d-cite> assign ranked importance values to parts of a graph that are relevant for a task. Because realistic and challenging graph problems can be  generated synthetically, GNNs can serve as a rigorous and repeatable testbed for evaluating attribution techniques <d-cite key="NEURIPS2020_6054"></d-cite>.</p>
<figure><img src="graph_xai.bce4532f.png">
<figcaption>Schematic of some explanability techniques on graphs. Attributions assign ranked values to graph attributes. Rankings can be used as a basis to extract connected subgraphs that might be relevant to a task.
</figcaption></figure>


<h3 id="generative-modelling">Generative modelling</h3>
<p>Besides learning predictive models on graphs, we might also care about learning a generative model for graphs. With a generative model we can generate new graphs by sampling from a learned distribution or by completing a graph given a starting point. A relevant application is in the design of new drugs, where novel molecular graphs with specific properties are desired as candidates to treat a disease.</p>
<p>A key challenge with graph generative models lies in modelling the topology of a graph, which can vary dramatically in size and has $N_{nodes}^2$ terms. One solution lies in modelling the adjacency matrix directly like an image with an autoencoder framework.<d-cite key="Kipf2016-ky"></d-cite> The prediction of the presence or absence of an edge is treated as a binary classification task. The $N_{nodes}^2$ term can be avoided by only predicting known edges and a subset of the edges that are not present. The graphVAE learns to model positive patterns of connectivity and some patterns of non-connectivity in the adjacency matrix.</p>
<p>Another approach is to build a graph sequentially, by starting with a graph and applying discrete actions such as addition or subtraction of nodes and edges iteratively. To avoid estimating a gradient for discrete actions we can use a policy gradient. This has been done via an auto-regressive model, such a RNN<d-cite key="You2018-vx"></d-cite>, or in a reinforcement learning scenario.<d-cite key="Zhou2019-ko"></d-cite> Furthermore, sometimes graphs can be modeled as just sequences with grammar elements.<d-cite key="Krenn2019-gg"></d-cite><d-cite key="Goyal2020-wl"></d-cite></p>
<!--[TODO: Image sketch of a graph generation]-->

<h2 id="final-thoughts">Final thoughts</h2>
<p>Graphs are a powerful and rich structured data type that have strengths and challenges that are very different from those of images and text. In this article, we have outlined some of the milestones that researchers have come up with in building neural network based models that process graphs. We have walked through some of the important design choices that must be made when using these architectures, and hopefully the GNN playground can give an intuition on what the empirical results of these design choices are. The success of GNNs in recent years creates a great opportunity for a wide range of new problems, and we are excited to see what the field will bring. 
</p></d-article><p></p>
 <d-appendix>

<h3 id="acknowledgments">Acknowledgments</h3>
<p>We are deeply grateful to Andy Coenen, Brian Lee, Chaitanya K. Joshi, Ed Chi, Humza Iqbal, Fernanda Viegas, Jasper Snoek, Jennifer Wei, Martin Wattenberg, Patricia Robinson, Wesley Qian and Yiliu Wang for their helpful feedback and suggestions, and to Michael Terry for his code reviews. </p>
<p>Many of our GNN architecture diagrams are based on the Graph Nets diagram <d-cite key="Battaglia2018-pi"></d-cite>.</p>
<h3 id="author-contributions">Author Contributions</h3>
<p>All authors contributed to writing.</p>
<p>Adam Pearce and Emily Reif made the interactive diagrams and set up the figure aesthetics. 
Benjamin Sanchez-Lengeling and Emily Reif made some of the initial image sketches.
Alexander B. Wiltschko provided editing and writing guidance. </p>
<h3 id="discussion-and-review">Discussion and Review</h3>
<p><a href="https://github.com/distillpub/post--gnn-intro/issues/1">Review #1 - Chaitanya K. Joshi</a>  <br>
<a href="https://github.com/distillpub/post--gnn-intro/issues/2">Review #2 - Patricia Robinson</a> <br>
<a href="https://github.com/distillpub/post--gnn-intro/issues/3">Review #3 - Humza Iqbal</a></p>
<d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="daigavane2021understanding"><span class="title">Understanding Convolutions on Graphs</span> <br>Daigavane, A., Ravindran, B. and Aggarwal, G., 2021. Distill.  <a href="https://doi.org/10.23915/distill.00032" style="text-decoration:inherit;">DOI: 10.23915/distill.00032</a></li><li id="Scarselli2009-ku"><span class="title">The Graph Neural Network Model</span> <br>Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M. and Monfardini, G., 2009. IEEE Transactions on Neural Networks, Vol 20(1), pp. 61--80. </li><li id="Stokes2020-az"><span class="title">A Deep Learning Approach to Antibiotic Discovery</span> <br>Stokes, J.M., Yang, K., Swanson, K., Jin, W., Cubillos-Ruiz, A., Donghia, N.M., MacNair, C.R., French, S., Carfrae, L.A., Bloom-Ackermann, Z., Tran, V.M., Chiappino-Pepe, A., Badran, A.H., Andrews, I.W., Chory, E.J., Church, G.M., Brown, E.D., Jaakkola, T.S., Barzilay, R. and Collins, J.J., 2020. Cell, Vol 181(2), pp. 475--483. </li><li id="Sanchez-Gonzalez2020-yo"><span class="title">Learning to simulate complex physics with graph networks</span> <br>Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J. and Battaglia, P.W., 2020. </li><li id="Monti2019-tf"><span class="title">Fake News Detection on Social Media using Geometric Deep Learning</span> <br>Monti, F., Frasca, F., Eynard, D., Mannion, D. and Bronstein, M.M., 2019. </li><li id="undated-sy"><span class="title">Traffic prediction with advanced Graph Neural Networks</span> <br>*, O.L. and Perez, L.. </li><li id="Eksombatchai2017-il"><span class="title">Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in {Real-Time}</span> <br>Eksombatchai, C., Jindal, P., Liu, J.Z., Liu, Y., Sharma, R., Sugnet, C., Ulrich, M. and Leskovec, J., 2017. </li><li id="Duvenaud2015-yc"><span class="title">Convolutional Networks on Graphs for Learning Molecular Fingerprints</span> <br>Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J., Gomez-Bombarelli, R., Hirzel, T., Aspuru-Guzik, A. and Adams, R.P., 2015. </li><li id="Mikolov2013-vr"><span class="title">Distributed Representations of Words and Phrases and their Compositionality</span> <br>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. and Dean, J., 2013. </li><li id="Devlin2018-mi"><span class="title">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span> <br>Devlin, J., Chang, M., Lee, K. and Toutanova, K., 2018. </li><li id="Pennington2014-kg"><span class="title">Glove: Global Vectors for Word Representation</span> <br>Pennington, J., Socher, R. and Manning, C., 2014. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). </li><li id="Allamanis2017-kz"><span class="title">Learning to Represent Programs with Graphs</span> <br>Allamanis, M., Brockschmidt, M. and Khademi, M., 2017. </li><li id="Lample2019-jg"><span class="title">Deep Learning for Symbolic Mathematics</span> <br>Lample, G. and Charton, F., 2019. </li><li id="Kunegis2013-er"><span class="title">KONECT</span> <br>Kunegis, J., 2013. Proceedings of the 22nd International Conference on World Wide Web - WWW '13 Companion. </li><li id="Zachary1977-jg"><span class="title">An Information Flow Model for Conflict and Fission in Small Groups</span> <br>Zachary, W.W., 1977. J. Anthropol. Res., Vol 33(4), pp. 452--473. The University of Chicago Press.</li><li id="Mena2018-ce"><span class="title">Learning Latent Permutations with Gumbel-Sinkhorn Networks</span> <br>Mena, G., Belanger, D., Linderman, S. and Snoek, J., 2018. </li><li id="Murphy2018-fz"><span class="title">Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs</span> <br>Murphy, R.L., Srinivasan, B., Rao, V. and Ribeiro, B., 2018. </li><li id="Gilmer2017-no"><span class="title">Neural Message Passing for Quantum Chemistry</span> <br>Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O. and Dahl, G.E., 2017. Proceedings of the 34th International Conference on Machine Learning, Vol 70, pp. 1263--1272. PMLR.</li><li id="Battaglia2018-pi"><span class="title">Relational inductive biases, deep learning, and graph networks</span> <br>Battaglia, P.W., Hamrick, J.B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick, M., Vinyals, O., Li, Y. and Pascanu, R., 2018. </li><li id="Zaheer2017-uc"><span class="title">Deep Sets</span> <br>Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. and Smola, A., 2017. </li><li id="Kearnes2016-rl"><span class="title">Molecular graph convolutions: moving beyond fingerprints</span> <br>Kearnes, S., McCloskey, K., Berndl, M., Pande, V. and Riley, P., 2016. J. Comput. Aided Mol. Des., Vol 30(8), pp. 595--608. </li><li id="Dumoulin2018-tb"><span class="title">Feature-wise transformations</span> <br>Dumoulin, V., Perez, E., Schucher, N., Strub, F., Vries, H.d., Courville, A. and Bengio, Y., 2018. Distill, Vol 3(7), pp. e11. </li><li id="Sanchez-Lengeling2020-qq"><span class="title">Leffingwell Odor Dataset</span> <br>Sanchez-Lengeling, B., Wei, J.N., Lee, B.K., Gerkin, R.C., Aspuru-Guzik, A. and Wiltschko, A.B., 2020. </li><li id="Sanchez-Lengeling2019-vs"><span class="title">Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules</span> <br>Sanchez-Lengeling, B., Wei, J.N., Lee, B.K., Gerkin, R.C., Aspuru-Guzik, A. and Wiltschko, A.B., 2019. </li><li id="Dwivedi2020-xm"><span class="title">Benchmarking Graph Neural Networks</span> <br>Dwivedi, V.P., Joshi, C.K., Laurent, T., Bengio, Y. and Bresson, X., 2020. </li><li id="You2020-vk"><span class="title">Design Space for Graph Neural Networks</span> <br>You, J., Ying, R. and Leskovec, J., 2020. </li><li id="Corso2020-py"><span class="title">Principal Neighbourhood Aggregation for Graph Nets</span> <br>Corso, G., Cavalleri, L., Beaini, D., Lio, P. and Velickovic, P., 2020. </li><li id="Markowitz2021-rn"><span class="title">Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning</span> <br>Markowitz, E., Balasubramanian, K., Mirtaheri, M., Abu-El-Haija, S., Perozzi, B., Ver Steeg, G. and Galstyan, A., 2021. </li><li id="Du2019-hr"><span class="title">Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels</span> <br>Du, S.S., Hou, K., Poczos, B., Salakhutdinov, R., Wang, R. and Xu, K., 2019. </li><li id="Xu2018-hq"><span class="title">Representation Learning on Graphs with Jumping Knowledge Networks</span> <br>Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K. and Jegelka, S., 2018. </li><li id="Velickovic2019-io"><span class="title">Neural Execution of Graph Algorithms</span> <br>Velickovic, P., Ying, R., Padovano, M., Hadsell, R. and Blundell, C., 2019. </li><li id="Harary1969-qo"><span class="title">Graph Theory</span> <br>Harary, F., 1969. </li><li id="Poulovassilis1994-bt"><span class="title">A nested-graph model for the representation and manipulation of complex objects</span> <br>Poulovassilis, A. and Levene, M., 1994. ACM Transactions on Information Systems, Vol 12(1), pp. 35--68. </li><li id="Zitnik2018-uk"><span class="title">Modeling polypharmacy side effects with graph convolutional networks</span> <br>Zitnik, M., Agrawal, M. and Leskovec, J., 2018. Bioinformatics, Vol 34(13), pp. i457--i466. </li><li id="Stocker2020-tr"><span class="title">Machine learning in chemical reaction space</span> <br>Stocker, S., Csanyi, G., Reuter, K. and Margraf, J.T., 2020. Nat. Commun., Vol 11(1), pp. 5505. </li><li id="Berge1976-ss"><span class="title">Graphs and Hypergraphs</span> <br>Berge, C., 1976. Elsevier.</li><li id="Yadati2018-de"><span class="title">HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs</span> <br>Yadati, N., Nimishakavi, M., Yadav, P., Nitin, V., Louis, A. and Talukdar, P., 2018. </li><li id="Zhong2020-mv"><span class="title">Hierarchical Message-Passing Graph Neural Networks</span> <br>Zhong, Z., Li, C. and Pang, J., 2020. </li><li id="Rozemberczki2020-lq"><span class="title">Little Ball of Fur</span> <br>Rozemberczki, B., Kiss, O. and Sarkar, R., 2020. Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. </li><li id="Leskovec2006-st"><span class="title">Sampling from large graphs</span> <br>Leskovec, J. and Faloutsos, C., 2006. Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06. </li><li id="Hubler2008-us"><span class="title">Metropolis Algorithms for Representative Subgraph Sampling</span> <br>Hubler, C., Kriegel, H., Borgwardt, K. and Ghahramani, Z., 2008. 2008 Eighth IEEE International Conference on Data Mining. </li><li id="Chiang2019-yh"><span class="title">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</span> <br>Chiang, W., Liu, X., Si, S., Li, Y., Bengio, S. and Hsieh, C., 2019. </li><li id="Zeng2019-eh"><span class="title">GraphSAINT: Graph Sampling Based Inductive Learning Method</span> <br>Zeng, H., Zhou, H., Srivastava, A., Kannan, R. and Prasanna, V., 2019. </li><li id="Xu2018-sf"><span class="title">How Powerful are Graph Neural Networks?</span> <br>Xu, K., Hu, W., Leskovec, J. and Jegelka, S., 2018. </li><li id="Skianis2019-ds"><span class="title">Rep the Set: Neural Networks for Learning Set Representations</span> <br>Skianis, K., Nikolentzos, G., Limnios, S. and Vazirgiannis, M., 2019. </li><li id="Pattanaik2020-jj"><span class="title">Message Passing Networks for Molecules with Tetrahedral Chirality</span> <br>Pattanaik, L., Ganea, O., Coley, I., Jensen, K.F., Green, W.H. and Coley, C.W., 2020. </li><li id="Liu2018-kf"><span class="title">N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules</span> <br>Liu, S., Demirel, M.F. and Liang, Y., 2018. </li><li id="Monti2018-ov"><span class="title">Dual-Primal Graph Convolutional Networks</span> <br>Monti, F., Shchur, O., Bojchevski, A., Litany, O., Gunnemann, S. and Bronstein, M.M., 2018. </li><li id="noauthor_undated-qq"><span class="title">Viewing matrices &amp; probability as graphs</span> <br>Bradley, T.. </li><li id="Bapat2014-fk"><span class="title">Graphs and Matrices</span> <br>Bapat, R.B., 2014. Springer.</li><li id="Bollobas2013-uk"><span class="title">Modern Graph Theory</span> <br>Bollobas, B., 2013. Springer Science &amp; Business Media.</li><li id="Vaswani2017-as"><span class="title">Attention Is All You Need</span> <br>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017. </li><li id="Velickovic2017-hf"><span class="title">Graph Attention Networks</span> <br>Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P. and Bengio, Y., 2017. </li><li id="Lee2018-ti"><span class="title">Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</span> <br>Lee, J., Lee, Y., Kim, J., Kosiorek, A.R., Choi, S. and Teh, Y.W., 2018. </li><li id="Joshi2020-ze"><span class="title">Transformers are Graph Neural Networks</span> <br>Joshi, C., 2020. NTU Graph Deep Learning Lab.</li><li id="McCloskey2018-ml"><span class="title">Using Attribution to Decode Dataset Bias in Neural Network Models for Chemistry</span> <br>McCloskey, K., Taly, A., Monti, F., Brenner, M.P. and Colwell, L., 2018. </li><li id="Ying2019-gk"><span class="title">GNNExplainer: Generating Explanations for Graph Neural Networks</span> <br>Ying, Z., Bourgeois, D., You, J., Zitnik, M. and Leskovec, J., 2019. Advances in Neural Information Processing Systems, Vol 32, pp. 9244--9255. Curran Associates, Inc.</li><li id="Pope2019-py"><span class="title">Explainability Methods for Graph Convolutional Neural Networks</span> <br>Pope, P.E., Kolouri, S., Rostami, M., Martin, C.E. and Hoffmann, H., 2019. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). </li><li id="NEURIPS2020_6054"><span class="title">Evaluating Attribution for Graph Neural Networks</span>  â€‚<a href="https://papers.nips.cc/paper/2020/hash/417fbbf2e9d5a28a855a11894b2e795a-Abstract.html">[HTML]</a><br>Sanchez-Lengeling, B., Wei, J., Lee, B., Reif, E., Qian, W., Wang, Y., McCloskey, K.J., Colwell, L. and Wiltschko, A.B., 2020. Advances in Neural Information Processing Systems 33. </li><li id="Kipf2016-ky"><span class="title">Variational Graph Auto-Encoders</span> <br>Kipf, T.N. and Welling, M., 2016. </li><li id="You2018-vx"><span class="title">GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models</span> <br>You, J., Ying, R., Ren, X., Hamilton, W.L. and Leskovec, J., 2018. </li><li id="Zhou2019-ko"><span class="title">Optimization of Molecules via Deep Reinforcement Learning</span> <br>Zhou, Z., Kearnes, S., Li, L., Zare, R.N. and Riley, P., 2019. Sci. Rep., Vol 9(1), pp. 1--10. Nature Publishing Group.</li><li id="Krenn2019-gg"><span class="title">Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation</span> <br>Krenn, M., Hase, F., Nigam, A., Friederich, P. and Aspuru-Guzik, A., 2019. </li><li id="Goyal2020-wl"><span class="title">GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation</span> <br>Goyal, N., Jain, H.V. and Ranu, S., 2020. </li></ol></d-citation-list>
<d-footnote-list></d-footnote-list><p></p>
<distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>

    <h3 id="updates-and-corrections">Updates and Corrections</h3>
    <p>
    If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--gnn-intro/issues/new">create an issue on GitHub</a>. </p>
    
    <h3 id="reuse">Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--gnn-intro">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources donâ€™t fall under this license and can be recognized by a note in their caption: â€œFigure from â€¦â€.</p>
    
    <h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Sanchez-Lengeling, et al., "A Gentle Introduction to Graph Neural Networks", Distill, 2021.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{sanchez-lengeling2021a,
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  title = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/gnn-intro},
  doi = {10.23915/distill.00033}
}</pre>
    </distill-appendix></d-appendix>



<p><d-bibliography><script type="text/json">[["Monti2018-ov",{"title":"Dual-Primal Graph Convolutional Networks","author":"Monti, Federico and Shchur, Oleksandr and Bojchevski, Aleksandar and Litany, Or and Gunnemann, Stephan and Bronstein, Michael M","abstract":"In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.","month":"jun","year":"2018","eprint":"1806.00770","type":"ARTICLE"}],["Battaglia2018-pi",{"title":"Relational inductive biases, deep learning, and graph networks","author":"Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan","abstract":"Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ``hand-engineering'' and ``end-to-end'' learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.","month":"jun","year":"2018","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1806.01261","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Corso2020-py",{"title":"Principal Neighbourhood Aggregation for Graph Nets","author":"Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Lio, Pietro and Velickovic, Petar","abstract":"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.","month":"apr","year":"2020","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2004.05718","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Poulovassilis1994-bt",{"title":"A nested-graph model for the representation and manipulation of complex objects","author":"Poulovassilis, Alexandra and Levene, Mark","journal":"ACM Transactions on Information Systems","volume":"12","number":"1","pages":"35--68","year":"1994","type":"MISC"}],["Gao2019-lf",{"title":"Graph U-Nets","author":"Gao, Hongyang and Ji, Shuiwang","abstract":"We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.","month":"may","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1905.05178","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Pope2019-py",{"title":"Explainability Methods for Graph Convolutional Neural Networks","author":"Pope, Phillip E and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E and Hoffmann, Heiko","journal":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","year":"2019","type":"MISC"}],["Zachary1977-jg",{"title":"An Information Flow Model for Conflict and Fission in Small Groups","author":"Zachary, Wayne W","abstract":"Data from a voluntary association are used to construct a new formal model for a traditional anthropological problem, fission in small groups. The process leading to fission is viewed as an unequal flow of sentiments and information across the ties in a social network. This flow is unequal because it is uniquely constrained by the contextual range and sensitivity of each relationship in the network. The subsequent differential sharing of sentiments leads to the formation of subgroups with more internal stability than the group as a whole, and results in fission. The Ford-Fulkerson labeling algorithm allows an accurate prediction of membership in the subgroups and of the locus of the fission to be made from measurements of the potential for information flow across each edge in the network. Methods for measurement of potential information flow are discussed, and it is shown that all appropriate techniques will generate the same predictions.","journal":"J. Anthropol. Res.","publisher":"The University of Chicago Press","volume":"33","number":"4","pages":"452--473","month":"dec","year":"1977","type":"ARTICLE"}],["Duvenaud2015-yc",{"title":"Convolutional Networks on Graphs for Learning Molecular Fingerprints","author":"Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gomez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P","abstract":"We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.","month":"sep","year":"2015","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1509.09292","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Pennington2014-kg",{"title":"Glove: Global Vectors for Word Representation","author":"Pennington, Jeffrey and Socher, Richard and Manning, Christopher","journal":"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)","year":"2014","type":"MISC"}],["Velickovic2017-hf",{"title":"Graph Attention Networks","author":"Velickovic, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","month":"oct","year":"2017","eprint":"1710.10903","type":"ARTICLE"}],["Vaswani2017-as",{"title":"Attention Is All You Need","author":"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","month":"jun","year":"2017","eprint":"1706.03762","type":"ARTICLE"}],["Lample2019-jg",{"title":"Deep Learning for Symbolic Mathematics","author":"Lample, Guillaume and Charton, Francois","abstract":"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.","month":"dec","year":"2019","eprint":"1912.01412","type":"ARTICLE"}],["McCloskey2018-ml",{"title":"Using Attribution to Decode Dataset Bias in Neural Network Models for Chemistry","author":"McCloskey, Kevin and Taly, Ankur and Monti, Federico and Brenner, Michael P and Colwell, Lucy","abstract":"Deep neural networks have achieved state of the art accuracy at classifying molecules with respect to whether they bind to specific protein targets. A key breakthrough would occur if these models could reveal the fragment pharmacophores that are causally involved in binding. Extracting chemical details of binding from the networks could potentially lead to scientific discoveries about the mechanisms of drug actions. But doing so requires shining light into the black box that is the trained neural network model, a task that has proved difficult across many domains. Here we show how the binding mechanism learned by deep neural network models can be interrogated, using a recently described attribution method. We first work with carefully constructed synthetic datasets, in which the 'fragment logic' of binding is fully known. We find that networks that achieve perfect accuracy on held out test datasets still learn spurious correlations due to biases in the datasets, and we are able to exploit this non-robustness to construct adversarial examples that fool the model. The dataset bias makes these models unreliable for accurately revealing information about the mechanisms of protein-ligand binding. In light of our findings, we prescribe a test that checks for dataset bias given a hypothesis. If the test fails, it indicates that either the model must be simplified or regularized and/or that the training dataset requires augmentation.","month":"nov","year":"2018","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1811.11310","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Rozemberczki2020-lq",{"title":"Little Ball of Fur","author":"Rozemberczki, Benedek and Kiss, Oliver and Sarkar, Rik","journal":"Proceedings of the 29th ACM International Conference on Information & Knowledge Management","year":"2020","type":"MISC"}],["Berge1976-ss",{"title":"Graphs and Hypergraphs","author":"Berge, Claude","publisher":"Elsevier","year":"1976","language":"en","type":"BOOK"}],["Harary1969-qo",{"title":"Graph Theory","author":"Harary, Frank","year":"1969","type":"MISC"}],["Zaheer2017-uc",{"title":"Deep Sets","author":"Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander","abstract":"We study the problem of designing models for machine learning tasks defined on \\textbackslashemph\\{sets\\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\textbackslashcite\\{poczos13aistats\\}, to anomaly detection in piezometer data of embankment dams \\textbackslashcite\\{Jung15Exploration\\}, to cosmology \\textbackslashcite\\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.","month":"mar","year":"2017","eprint":"1703.06114","type":"ARTICLE"}],["Kunegis2013-er",{"title":"KONECT","author":"Kunegis, Jerome","journal":"Proceedings of the 22nd International Conference on World Wide Web - WWW '13 Companion","year":"2013","type":"MISC"}],["Zitnik2018-uk",{"title":"Modeling polypharmacy side effects with graph convolutional networks","author":"Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure","abstract":"Motivation: The use of drug combinations, termed polypharmacy, is common to treat patients with complex diseases or co-existing conditions. However, a major consequence of polypharmacy is a much higher risk of adverse side effects for the patient. Polypharmacy side effects emerge because of drug-drug interactions, in which activity of one drug may change, favorably or unfavorably, if taken with another drug. The knowledge of drug interactions is often limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. Discovering polypharmacy side effects thus remains an important challenge with significant implications for patient mortality and morbidity. Results: Here, we present Decagon, an approach for modeling polypharmacy side effects. The approach constructs a multimodal graph of protein-protein interactions, drug-protein target interactions and the polypharmacy side effects, which are represented as drug-drug interactions, where each side effect is an edge of a different type. Decagon is developed specifically to handle such multimodal graphs with a large number of edge types. Our approach develops a new graph convolutional neural network for multirelational link prediction in multimodal networks. Unlike approaches limited to predicting simple drug-drug interaction values, Decagon can predict the exact side effect, if any, through which a given drug combination manifests clinically. Decagon accurately predicts polypharmacy side effects, outperforming baselines by up to 69\\%. We find that it automatically learns representations of side effects indicative of co-occurrence of polypharmacy in patients. Furthermore, Decagon models particularly well polypharmacy side effects that have a strong molecular basis, while on predominantly non-molecular side effects, it achieves good performance because of effective sharing of model parameters across edge types. Decagon opens up opportunities to use large pharmacogenomic and patient population data to flag and prioritize polypharmacy side effects for follow-up analysis via formal pharmacological studies. Availability and implementation: Source code and preprocessed datasets are at: http://snap.stanford.edu/decagon.","journal":"Bioinformatics","volume":"34","number":"13","pages":"i457--i466","month":"jul","year":"2018","language":"en","type":"ARTICLE"}],["Kearnes2016-rl",{"title":"Molecular graph convolutions: moving beyond fingerprints","author":"Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick","abstract":"Molecular ``fingerprints'' encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph-atoms, bonds, distances, etc.-which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.","journal":"J. Comput. Aided Mol. Des.","volume":"30","number":"8","pages":"595--608","month":"aug","year":"2016","keywords":"Artificial neural networks; Deep learning; Machine learning; Molecular descriptors; Virtual screening;references.bib","language":"en","type":"ARTICLE"}],["Kipf2016-ky",{"title":"Variational Graph Auto-Encoders","author":"Kipf, Thomas N and Welling, Max","abstract":"We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.","month":"nov","year":"2016","eprint":"1611.07308","type":"ARTICLE"}],["You2018-vx",{"title":"GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models","author":"You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L and Leskovec, Jure","abstract":"Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.","month":"feb","year":"2018","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1802.08773","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Devlin2018-mi",{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\\% (7.7\\% point absolute improvement), MultiNLI accuracy to 86.7\\% (4.6\\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","month":"oct","year":"2018","eprint":"1810.04805","type":"ARTICLE"}],["Liao2019-kf",{"title":"Efficient Graph Generation with Graph Recurrent Attention Networks","author":"Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S","abstract":"We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.","month":"oct","year":"2019","eprint":"1910.00760","type":"ARTICLE"}],["Dumoulin2018-tb",{"title":"Feature-wise transformations","author":"Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm de and Courville, Aaron and Bengio, Yoshua","abstract":"A simple and surprisingly effective family of conditioning mechanisms.","journal":"Distill","volume":"3","number":"7","pages":"e11","month":"jul","year":"2018","type":"ARTICLE"}],["Lee2018-ti",{"title":"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks","author":"Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R and Choi, Seungjin and Teh, Yee Whye","abstract":"Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.","month":"oct","year":"2018","eprint":"1810.00825","type":"ARTICLE"}],["Skianis2019-ds",{"title":"Rep the Set: Neural Networks for Learning Set Representations","author":"Skianis, Konstantinos and Nikolentzos, Giannis and Limnios, Stratis and Vazirgiannis, Michalis","abstract":"In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms.","month":"apr","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1904.01962","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Gilmer2017-no",{"title":"Neural Message Passing for Quantum Chemistry","booktitle":"Proceedings of the 34th International Conference on Machine Learning","author":"Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E","editor":"Precup, Doina and Teh, Yee Whye","abstract":"Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.","publisher":"PMLR","volume":"70","pages":"1263--1272","series":"Proceedings of Machine Learning Research","year":"2017","address":"International Convention Centre, Sydney, Australia","type":"INPROCEEDINGS"}],["Allamanis2017-kz",{"title":"Learning to Represent Programs with Graphs","author":"Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud","abstract":"Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.","month":"nov","year":"2017","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1711.00740","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Mena2018-ce",{"title":"Learning Latent Permutations with Gumbel-Sinkhorn Networks","author":"Mena, Gonzalo and Belanger, David and Linderman, Scott and Snoek, Jasper","abstract":"Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator. Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator. With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings. We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.","month":"feb","year":"2018","eprint":"1802.08665","type":"ARTICLE"}],["Scarselli2009-ku",{"title":"The Graph Neural Network Model","author":"Scarselli, F and Gori, M and Tsoi, Ah Chung and Hagenbuchner, M and Monfardini, G","journal":"IEEE Transactions on Neural Networks","volume":"20","number":"1","pages":"61--80","year":"2009","type":"MISC"}],["Krenn2019-gg",{"title":"Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation","author":"Krenn, Mario and Hase, Florian and Nigam, Akshatkumar and Friederich, Pascal and Aspuru-Guzik, Alan","abstract":"The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering -- generally denoted as inverse design -- was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.","month":"may","year":"2019","eprint":"1905.13741","type":"ARTICLE"}],["Goyal2020-wl",{"title":"GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation","author":"Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan","abstract":"Graph generative models have been extensively studied in the data mining literature. While traditional techniques are based on generating structures that adhere to a pre-decided distribution, recent techniques have shifted towards learning this distribution directly from the data. While learning-based approaches have imparted significant improvement in quality, some limitations remain to be addressed. First, learning graph distributions introduces additional computational overhead, which limits their scalability to large graph databases. Second, many techniques only learn the structure and do not address the need to also learn node and edge labels, which encode important semantic information and influence the structure itself. Third, existing techniques often incorporate domain-specific rules and lack generalizability. Fourth, the experimentation of existing techniques is not comprehensive enough due to either using weak evaluation metrics or focusing primarily on synthetic or small datasets. In this work, we develop a domain-agnostic technique called GraphGen to overcome all of these limitations. GraphGen converts graphs to sequences using minimum DFS codes. Minimum DFS codes are canonical labels and capture the graph structure precisely along with the label information. The complex joint distributions between structure and semantic labels are learned through a novel LSTM architecture. Extensive experiments on million-sized, real graph datasets show GraphGen to be 4 times faster on average than state-of-the-art techniques while being significantly better in quality across a comprehensive set of 11 different metrics. Our code is released at https://github.com/idea-iitd/graphgen.","month":"jan","year":"2020","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2001.08184","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Ying2019-gk",{"title":"GNNExplainer: Generating Explanations for Graph Neural Networks","booktitle":"Advances in Neural Information Processing Systems","author":"Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure","editor":"Wallach, H and Larochelle, H and Beygelzimer, A and d\\textbackslashtextquotesingle Alche-Buc, F and Fox, E and Garnett, R","publisher":"Curran Associates, Inc.","volume":"32","pages":"9244--9255","year":"2019","type":"INPROCEEDINGS"}],["Sanchez-Lengeling2020-qq",{"title":"Leffingwell Odor Dataset","author":"Sanchez-Lengeling, Benjamin and Wei, Jennifer N and Lee, Brian K and Gerkin, Richard C and Aspuru-Guzik, Alan and Wiltschko, Alexander B","abstract":"Predicting properties of molecules is an area of growing research in machine learning, particularly as models for learning from graph-valued inputs improve in sophistication and robustness. A molecular property prediction problem that has received comparatively little attention during this surge in research activity is building Structure-Odor Relationships (SOR) models (as opposed to Quantitative Structure-Activity Relationships, a term from medicinal chemistry). This is a 70+ year-old problem straddling chemistry, physics, neuroscience, and machine learning. To spur development on the SOR problem, we curated and cleaned a dataset of 3523 molecules associated with expert-labeled odor descriptors from the Leffingwell PMP 2001 database. We provide featurizations of all molecules in the dataset using bit-based and count-based fingerprints, Mordred molecular descriptors, and the embeddings from our trained GNN model (Sanchez-Lengeling et al., 2019). This dataset is comprised of two files: leffingwell\\_data.csv: this contains molecular structures, and what they smell like, along with train, test, and cross-validation splits. More detail on the file structure is found in leffingwell\\_readme.pdf. leffingwell\\_embeddings.npz: this contains several featurizations of the molecules in the dataset. leffingwell\\_readme.pdf: a more detailed description of the data and its provenance, including expected performance metrics. LICENSE: a copy of the CC-BY-NC license language. The dataset, and all associated features, is freely available for research use under the CC-BY-NC license.","month":"oct","year":"2020","keywords":"machine learning; artificial intelligence; olfaction; neuroscience; chemistry; scent; fragrance","type":"MISC"}],["Hubler2008-us",{"title":"Metropolis Algorithms for Representative Subgraph Sampling","author":"Hubler, Christian and Kriegel, Hans-Peter and Borgwardt, Karsten and Ghahramani, Zoubin","journal":"2008 Eighth IEEE International Conference on Data Mining","year":"2008","type":"MISC"}],["Mikolov2013-vr",{"title":"Distributed Representations of Words and Phrases and their Compositionality","author":"Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey","abstract":"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ``Canada'' and ``Air'' cannot be easily combined to obtain ``Air Canada''. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","month":"oct","year":"2013","eprint":"1310.4546","type":"ARTICLE"}],["Xu2018-sf",{"title":"How Powerful are Graph Neural Networks?","author":"Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie","abstract":"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.","month":"oct","year":"2018","eprint":"1810.00826","type":"ARTICLE"}],["Liu2018-kf",{"title":"N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules","author":"Liu, Shengchao and Demirel, Mehmet Furkan and Liang, Yingyu","abstract":"Machine learning techniques have recently been adopted in various applications in medicine, biology, chemistry, and material engineering. An important task is to predict the properties of molecules, which serves as the main subroutine in many downstream applications such as virtual screening and drug design. Despite the increasing interest, the key challenge is to construct proper representations of molecules for learning algorithms. This paper introduces the N-gram graph, a simple unsupervised representation for molecules. The method first embeds the vertices in the molecule graph. It then constructs a compact representation for the graph by assembling the vertex embeddings in short walks in the graph, which we show is equivalent to a simple graph neural network that needs no training. The representations can thus be efficiently computed and then used with supervised learning methods for prediction. Experiments on 60 tasks from 10 benchmark datasets demonstrate its advantages over both popular graph neural networks and traditional representation methods. This is complemented by theoretical analysis showing its strong representation and prediction power.","month":"jun","year":"2018","eprint":"1806.09206","type":"ARTICLE"}],["Zhou2019-ko",{"title":"Optimization of Molecules via Deep Reinforcement Learning","author":"Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N and Riley, Patrick","abstract":"We present a framework, which we call Molecule Deep Q-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double Q-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100\\% chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. MolDQN achieves comparable or better performance against several other recently published algorithms for benchmark molecular optimization tasks. However, we also argue that many of these tasks are not representative of real optimization problems in drug discovery. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.","journal":"Sci. Rep.","publisher":"Nature Publishing Group","volume":"9","number":"1","pages":"1--10","month":"jul","year":"2019","language":"en","type":"ARTICLE"}],["Sanchez-Lengeling2019-vs",{"title":"Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules","author":"Sanchez-Lengeling, Benjamin and Wei, Jennifer N and Lee, Brian K and Gerkin, Richard C and Aspuru-Guzik, Alan and Wiltschko, Alexander B","abstract":"Predicting the relationship between a molecule's structure and its odor remains a difficult, decades-old task. This problem, termed quantitative structure-odor relationship (QSOR) modeling, is an important challenge in chemistry, impacting human nutrition, manufacture of synthetic fragrance, the environment, and sensory neuroscience. We propose the use of graph neural networks for QSOR, and show they significantly out-perform prior methods on a novel data set labeled by olfactory experts. Additional analysis shows that the learned embeddings from graph neural networks capture a meaningful odor space representation of the underlying relationship between structure and odor, as demonstrated by strong performance on two challenging transfer learning tasks. Machine learning has already had a large impact on the senses of sight and sound. Based on these early results with graph neural networks for molecular properties, we hope machine learning can eventually do for olfaction what it has already done for vision and hearing.","month":"oct","year":"2019","eprint":"1910.10685","type":"ARTICLE"}],["Murphy2018-fz",{"title":"Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs","author":"Murphy, Ryan L and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno","abstract":"We consider a simple and overarching representation for permutation-invariant functions of sequences (or multiset functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with $k$-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.","month":"nov","year":"2018","eprint":"1811.01900","type":"ARTICLE"}],["Leskovec2006-st",{"title":"Sampling from large graphs","author":"Leskovec, Jure and Faloutsos, Christos","journal":"Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06","year":"2006","type":"MISC"}],["NEURIPS2020_6054",{"title":"Evaluating Attribution for Graph Neural Networks","author":"Benjamin Sanchez-Lengeling and Jennifer Wei and Brian Lee and Emily Reif and Wesley Qian and Yiliu Wang and Kevin James McCloskey and Lucy Colwell and Alexander B Wiltschko","booktitle":"Advances in Neural Information Processing Systems 33","year":"2020","url":"https://papers.nips.cc/paper/2020/hash/417fbbf2e9d5a28a855a11894b2e795a-Abstract.html","type":"article"}],["Joshi2020-ze",{"title":"Transformers are Graph Neural Networks","author":"Joshi, Chaitanya","abstract":"Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications? Besides the obvious ones--recommendation systems at Pinterest, Alibaba and Twitter--a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm. Through this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I'll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress.","publisher":"NTU Graph Deep Learning Lab","month":"feb","year":"2020","howpublished":"\\url{https://graphdeeplearning.github.io/post/transformers-are-gnns/}","note":"Accessed: 2021-7-19","type":"MISC"}],["Eksombatchai2017-il",{"title":"Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in {Real-Time}","author":"Eksombatchai, Chantat and Jindal, Pranav and Liu, Jerry Zitao and Liu, Yuchen and Sharma, Rahul and Sugnet, Charles and Ulrich, Mark and Leskovec, Jure","abstract":"User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50\\% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58\\% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80\\% of all user engagement on Pinterest.","month":"nov","year":"2017","archivePrefix":"arXiv","primaryClass":"cs.IR","eprint":"1711.07601","archiveprefix":"arXiv","primaryclass":"cs.IR","type":"ARTICLE"}],["undated-sy",{"title":"Traffic prediction with advanced Graph Neural Networks","author":"*, Oliver Lange and Perez, Luis","abstract":"Working with our partners at Google Maps, we used advanced machine learning techniques including Graph Neural Networks, to improve the accuracy of real time ETAs by up to 50\\%.","howpublished":"\\url{https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks}","note":"Accessed: 2021-7-19","type":"MISC"}],["Monti2019-tf",{"title":"Fake News Detection on Social Media using Geometric Deep Learning","author":"Monti, Federico and Frasca, Fabrizio and Eynard, Davide and Mannion, Damon and Bronstein, Michael M","abstract":"Social media are nowadays one of the main news sources for millions of people around the globe due to their low cost, easy access and rapid dissemination. This however comes at the cost of dubious trustworthiness and significant risk of exposure to 'fake news', intentionally written to mislead the readers. Automatically detecting fake news poses challenges that defy existing content-based analysis approaches. One of the main reasons is that often the interpretation of the news requires the knowledge of political or social context or 'common sense', which current NLP algorithms are still missing. Recent studies have shown that fake and real news spread differently on social media, forming propagation patterns that could be harnessed for the automatic fake news detection. Propagation-based approaches have multiple advantages compared to their content-based counterparts, among which is language independence and better resilience to adversarial attacks. In this paper we show a novel automatic fake news detection model based on geometric deep learning. The underlying core algorithms are a generalization of classical CNNs to graphs, allowing the fusion of heterogeneous data such as content, user profile and activity, social graph, and news propagation. Our model was trained and tested on news stories, verified by professional fact-checking organizations, that were spread on Twitter. Our experiments indicate that social network structure and propagation are important features allowing highly accurate (92.7\\% ROC AUC) fake news detection. Second, we observe that fake news can be reliably detected at an early stage, after just a few hours of propagation. Third, we test the aging of our model on training and testing data separated in time. Our results point to the promise of propagation-based approaches for fake news detection as an alternative or complementary strategy to content-based approaches.","month":"feb","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.SI","eprint":"1902.06673","archiveprefix":"arXiv","primaryclass":"cs.SI","type":"ARTICLE"}],["Sanchez-Gonzalez2020-yo",{"title":"Learning to simulate complex physics with graph networks","author":"Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W","abstract":"Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term ``Graph Network-based Simulators'' (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.","month":"feb","year":"2020","copyright":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2002.09405","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Stokes2020-az",{"title":"A Deep Learning Approach to Antibiotic Discovery","author":"Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and Tran, Victoria M and Chiappino-Pepe, Anush and Badran, Ahmed H and Andrews, Ian W and Chory, Emma J and Church, George M and Brown, Eric D and Jaakkola, Tommi S and Barzilay, Regina and Collins, James J","journal":"Cell","volume":"181","number":"2","pages":"475--483","month":"apr","year":"2020","language":"en","type":"ARTICLE"}],["Dwivedi2020-xm",{"title":"Benchmarking Graph Neural Networks","author":"Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier","abstract":"Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.","month":"mar","year":"2020","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2003.00982","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["You2020-vk",{"title":"Design Space for Graph Neural Networks","author":"You, Jiaxuan and Ying, Rex and Leskovec, Jure","abstract":"The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.","month":"nov","year":"2020","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2011.08843","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Zhong2020-mv",{"title":"Hierarchical Message-Passing Graph Neural Networks","author":"Zhong, Zhiqiang and Li, Cheng-Te and Pang, Jun","abstract":"Graph Neural Networks (GNNs) have become a promising approach to machine learning with graphs. Since existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled. One is costly in encoding global information on the graph topology. The other is failing to model meso- and macro-level semantics hidden in the graph, such as the knowledge of institutes and research areas in an academic collaboration network. To deal with these two issues, we propose a novel Hierarchical Message-Passing Graph Neural Networks framework. The main idea is to generate a hierarchical structure that re-organises all nodes in a graph into multi-level clusters, along with intra- and inter-level edge connections. The derived hierarchy not only creates shortcuts connecting far-away nodes so that global information can be efficiently accessed via message passing but also incorporates meso- and macro-level semantics into the learning of node embedding. We present the first model to implement this hierarchical message-passing mechanism, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), based on hierarchical communities detected from the graph. Experiments conducted on eight datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection.","month":"sep","year":"2020","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2009.03717","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Yadati2018-de",{"title":"HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs","author":"Yadati, Naganand and Nimishakavi, Madhav and Yadav, Prateek and Nitin, Vikram and Louis, Anand and Talukdar, Partha","abstract":"In many real-world network datasets such as co-authorship, co-citation, email communication, etc., relationships are complex and go beyond pairwise. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. The obvious existence of such complex relationships in many real-world networks naturaly motivates the problem of learning with hypergraphs. A popular learning paradigm is hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabeled vertices in a hypergraph. Motivated by the fact that a graph convolutional network (GCN) has been effective for graph-based SSL, we propose HyperGCN, a novel GCN for SSL on attributed hypergraphs. Additionally, we show how HyperGCN can be used as a learning-based approach for combinatorial optimisation on NP-hard hypergraph problems. We demonstrate HyperGCN's effectiveness through detailed experimentation on real-world hypergraphs.","month":"sep","year":"2018","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1809.02589","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Stocker2020-tr",{"title":"Machine learning in chemical reaction space","author":"Stocker, Sina and Csanyi, Gabor and Reuter, Karsten and Margraf, Johannes T","abstract":"Chemical compound space refers to the vast set of all possible chemical compounds, estimated to contain 1060 molecules. While intractable as a whole, modern machine learning (ML) is increasingly capable of accurately predicting molecular properties in important subsets. Here, we therefore engage in the ML-driven study of even larger reaction space. Central to chemistry as a science of transformations, this space contains all possible chemical reactions. As an important basis for 'reactive' ML, we establish a first-principles database (Rad-6) containing closed and open-shell organic molecules, along with an associated database of chemical reaction energies (Rad-6-RE). We show that the special topology of reaction spaces, with central hub molecules involved in multiple reactions, requires a modification of existing compound space ML-concepts. Showcased by the application to methane combustion, we demonstrate that the learned reaction energies offer a non-empirical route to rationally extract reduced reaction networks for detailed microkinetic analyses.","journal":"Nat. Commun.","volume":"11","number":"1","pages":"5505","month":"oct","year":"2020","language":"en","type":"ARTICLE"}],["Chiang2019-yh",{"title":"Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks","author":"Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui","abstract":"Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16]. Our codes are publicly available at https://github.com/google-research/google-research/tree/master/cluster\\_gcn.","month":"may","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1905.07953","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Zeng2019-eh",{"title":"GraphSAINT: Graph Sampling Based Inductive Learning Method","author":"Zeng, Hanqing and Zhou, Hongkuan and Srivastava, Ajitesh and Kannan, Rajgopal and Prasanna, Viktor","abstract":"Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the ``neighbor explosion'' problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).","month":"jul","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1907.04931","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Markowitz2021-rn",{"title":"Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning","author":"Markowitz, Elan and Balasubramanian, Keshav and Mirtaheri, Mehrnoosh and Abu-El-Haija, Sami and Perozzi, Bryan and Ver Steeg, Greg and Galstyan, Aram","abstract":"Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g.message passing methods are run differently from node embedding ones. Despite their apparent differences, all these methods utilize the graph structure, and therefore, their learning can be approximated with stochastic graph traversals. We propose Graph Traversal via Tensor Functionals(GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs. GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly. With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries - with only a handful of lines of code for each method specialization. GTTF and its various GRL implementations are on: https://github.com/isi-usc-edu/gttf.","month":"feb","year":"2021","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"2102.04350","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Du2019-hr",{"title":"Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels","author":"Du, Simon S and Hou, Kangcheng and Poczos, Barnabas and Salakhutdinov, Ruslan and Wang, Ruosong and Xu, Keyulu","abstract":"While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.","month":"may","year":"2019","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1905.13192","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Xu2018-hq",{"title":"Representation Learning on Graphs with Jumping Knowledge Networks","author":"Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-Ichi and Jegelka, Stefanie","abstract":"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ``neighboring'' nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.","month":"jun","year":"2018","archivePrefix":"arXiv","primaryClass":"cs.LG","eprint":"1806.03536","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["Velickovic2019-io",{"title":"Neural Execution of Graph Algorithms","author":"Velickovic, Petar and Ying, Rex and Padovano, Matilde and Hadsell, Raia and Blundell, Charles","abstract":"Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.","month":"oct","year":"2019","archivePrefix":"arXiv","primaryClass":"stat.ML","eprint":"1910.10593","archiveprefix":"arXiv","primaryclass":"stat.ML","type":"ARTICLE"}],["noauthor_undated-qq",{"author":"Tai-Danae Bradley","title":"Viewing matrices & probability as graphs","howpublished":"\\url{https://www.math3ma.com/blog/matrices-probability-graphs}","type":"MISC"}],["Bapat2014-fk",{"title":"Graphs and Matrices","author":"Bapat, Ravindra B","abstract":"This new edition illustrates the power of linear algebra in the study of graphs. The emphasis on matrix techniques is greater than in other texts on algebraic graph theory. Important matrices associated with graphs (for example, incidence, adjacency and Laplacian matrices) are treated in detail.Presenting a useful overview of selected topics in algebraic graph theory, early chapters of the text focus on regular graphs, algebraic connectivity, the distance matrix of a tree, and its generalized version for arbitrary graphs, known as the resistance matrix. Coverage of later topics include Laplacian eigenvalues of threshold graphs, the positive definite completion problem and matrix games based on a graph.Such an extensive coverage of the subject area provides a welcome prompt for further exploration. The inclusion of exercises enables practical learning throughout the book.In the new edition, a new chapter is added on the line graph of a tree, while some results in Chapter 6 on Perron-Frobenius theory are reorganized.Whilst this book will be invaluable to students and researchers in graph theory and combinatorial matrix theory, it will also benefit readers in the sciences and engineering.","publisher":"Springer","month":"sep","year":"2014","language":"en","type":"BOOK"}],["Bollobas2013-uk",{"title":"Modern Graph Theory","author":"Bollobas, Bela","abstract":"The time has now come when graph theory should be part of the education of every serious student of mathematics and computer science, both for its own sake and to enhance the appreciation of mathematics as a whole. This book is an in-depth account of graph theory, written with such a student in mind; it reflects the current state of the subject and emphasizes connections with other branches of pure mathematics. The volume grew out of the author's earlier book, Graph Theory -- An Introductory Course, but its length is well over twice that of its predecessor, allowing it to reveal many exciting new developments in the subject. Recognizing that graph theory is one of several courses competing for the attention of a student, the book contains extensive descriptive passages designed to convey the flavor of the subject and to arouse interest. In addition to a modern treatment of the classical areas of graph theory such as coloring, matching, extremal theory, and algebraic graph theory, the book presents a detailed account of newer topics, including Szemer\\textbackslash'edi's Regularity Lemma and its use, Shelah's extension of the Hales-Jewett Theorem, the precise nature of the phase transition in a random graph process, the connection between electrical networks and random walks on graphs, and the Tutte polynomial and its cousins in knot theory. In no other branch of mathematics is it as vital to tackle and solve challenging exercises in order to master the subject. To this end, the book contains an unusually large number of well thought-out exercises: over 600 in total. Although some are straightforward, most of them are substantial, and others will stretch even the most able reader.","publisher":"Springer Science & Business Media","month":"dec","year":"2013","language":"en","type":"BOOK"}],["Pattanaik2020-jj",{"title":"Message Passing Networks for Molecules with Tetrahedral Chirality","author":"Pattanaik, Lagnajit and Ganea, Octavian-Eugen and Coley, Ian and Jensen, Klavs F and Green, William H and Coley, Connor W","abstract":"Molecules with identical graph connectivity can exhibit different physical and biological properties if they exhibit stereochemistry-a spatial structural characteristic. However, modern neural architectures designed for learning structure-property relationships from molecular structures treat molecules as graph-structured data and therefore are invariant to stereochemistry. Here, we develop two custom aggregation functions for message passing neural networks to learn properties of molecules with tetrahedral chirality, one common form of stereochemistry. We evaluate performance on synthetic data as well as a newly-proposed protein-ligand docking dataset with relevance to drug discovery. Results show modest improvements over a baseline sum aggregator, highlighting opportunities for further architecture development.","month":"nov","year":"2020","archivePrefix":"arXiv","primaryClass":"q-bio.QM","eprint":"2012.00094","archiveprefix":"arXiv","primaryclass":"q-bio.QM","type":"ARTICLE"}],["daigavane2021understanding",{"author":"Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav","title":"Understanding Convolutions on Graphs","journal":"Distill","year":"2021","note":"https://distill.pub/2021/understanding-gnns","doi":"10.23915/distill.00032","type":"article"}]]</script></d-bibliography></p>
<script src="post--gnn-intro.77de5100.js"></script>
<distill-footer>
<style>

:host {
  color: rgba(255, 255, 255, 0.5);
  font-weight: 300;
  padding: 2rem 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
  text-align: left;
  contain: content;
}

.footer-container .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}

.footer-container .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}

.footer-container .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}

.footer-container {
  grid-column: text;
}

.footer-container .nav {
  font-size: 0.9em;
  margin-top: 1.5em;
}

.footer-container .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
  text-decoration: none;
}

</style>

<div class="footer-container">

  <a href="/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a> is dedicated to clear explanations of machine learning

  <div class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/journal/">Submit</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/archive/">Archive</a>
    <a href="https://distill.pub/rss.xml">RSS</a>
    <a href="https://github.com/distillpub">GitHub</a>
    <a href="https://twitter.com/distillpub">Twitter</a>
    &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
  </div>

</div>

</distill-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body></html>